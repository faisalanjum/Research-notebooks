{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ************************************Imports************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inorder to connect to FEA, COT and WWPA Databases\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "def query_data(session,mapper,parameter_val=None,parameter=\"id\"):\n",
    "\n",
    "            session=session()\n",
    "            if type(parameter_val) == str:\n",
    "                q_res=session.query(mapper).filter( getattr(mapper,parameter) ==  parameter_val).all()\n",
    "                res=[q.toDict() for q in q_res]\n",
    "                return res\n",
    "\n",
    "            elif type(parameter_val) == list:\n",
    "                prm=getattr(mapper,parameter)\n",
    "                q_res=session.query(mapper).filter( prm.in_(parameter_val)).all()\n",
    "                res=[q.toDict() for q in q_res]\n",
    "                return res\n",
    "\n",
    "\n",
    "            elif parameter_val == None:\n",
    "                q_res=session.query(mapper).all()\n",
    "                res=[q.toDict() for q in q_res]\n",
    "                return res\n",
    "            \n",
    "            else:\n",
    "                print(\"enter valid par_value str and list is valid type \")\n",
    "\n",
    "def connect_to_database(db_url,db_schema):\n",
    "    try:\n",
    "        print(\"Connecting to database...\")\n",
    "        engine=create_engine(str(db_url),echo=False,connect_args={\"options\":\"-csearch_path={}\".format(db_schema)})\n",
    "        session=sessionmaker(engine,expire_on_commit=False)\n",
    "        print(\"Connection Established!\")\n",
    "        return session\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "   \n",
    "        raise ConnectionError(\"There is some error connecting to data base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _lumber_prices import *\n",
    "from _lumber_schema import *\n",
    "from adjusted_future import *\n",
    "\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ************************************Aggregation of All Variables************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Lumber dataframes (combined with Barchart Historicals) for front month and second month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from CWP database & excel files::  33%|███▎      | 1/3 [00:04<00:08,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Contract:1 00:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from CWP database & excel files::  67%|██████▋   | 2/3 [00:06<00:03,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Contract:2 00:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from CWP database & excel files:: 100%|██████████| 3/3 [00:09<00:00,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Contract:3 00:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting data from CWP database & excel files::  33%|███▎      | 1/3 [00:02<00:04,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Contract:1 00:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from CWP database & excel files::  67%|██████▋   | 2/3 [00:05<00:02,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Contract:2 00:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from CWP database & excel files:: 100%|██████████| 3/3 [00:07<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Contract:3 00:07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting data from CWP database & excel files::  33%|███▎      | 1/3 [00:02<00:05,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Contract:1 00:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from CWP database & excel files::  67%|██████▋   | 2/3 [00:05<00:02,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Contract:2 00:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from CWP database & excel files:: 100%|██████████| 3/3 [00:08<00:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Contract:3 00:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>openInterest</th>\n",
       "      <th>symbol</th>\n",
       "      <th>expirationDate</th>\n",
       "      <th>Days_to_expiry</th>\n",
       "      <th>cash_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-26</th>\n",
       "      <td>549.000000</td>\n",
       "      <td>511.799988</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LBU22</td>\n",
       "      <td>2022-09-15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-25</th>\n",
       "      <td>529.900024</td>\n",
       "      <td>495.200012</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>182.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>LBU22</td>\n",
       "      <td>2022-09-15</td>\n",
       "      <td>16.0</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969-10-02</th>\n",
       "      <td>80.500000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LBH70</td>\n",
       "      <td>1970-03-13</td>\n",
       "      <td>117.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969-10-01</th>\n",
       "      <td>80.250000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LBH70</td>\n",
       "      <td>1970-03-13</td>\n",
       "      <td>118.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  high         low       close  volume  openInterest symbol  \\\n",
       "2022-08-26  549.000000  511.799988  523.599976     NaN           NaN  LBU22   \n",
       "2022-08-25  529.900024  495.200012  520.000000   182.0         847.0  LBU22   \n",
       "1969-10-02   80.500000   79.500000   80.500000     NaN           NaN  LBH70   \n",
       "1969-10-01   80.250000   78.000000   80.000000     NaN           NaN  LBH70   \n",
       "\n",
       "           expirationDate  Days_to_expiry  cash_price  \n",
       "2022-08-26     2022-09-15            15.0       550.0  \n",
       "2022-08-25     2022-09-15            16.0       550.0  \n",
       "1969-10-02     1970-03-13           117.0         NaN  \n",
       "1969-10-01     1970-03-13           118.0         NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes 3 minutes to run\n",
    "Lum_1 = get_ctrct_mnth_df(ctrct_sequence=1)\n",
    "Lum_2 = get_ctrct_mnth_df(ctrct_sequence=2)\n",
    "Lum_3 = get_ctrct_mnth_df(ctrct_sequence=3)\n",
    "\n",
    "Lum_1.head(2).append(Lum_1.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching Logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using this switch as since it expires after the expiry we cant calculate Roll_Ratio requird to calculate the adjusted prices\n",
    "# 1. Never Switching - We still need to use previous close price on day of expiry since we need to calculate adj return when new contract starts \n",
    "# In this case just shifting list to the left - the last index won't matter as it is\n",
    "switch_list_1 = [item for sublist in np.where([Lum_1.index == Lum_1.expirationDate],1,0).tolist() for item in sublist]\n",
    "switch_list_1.append(switch_list_1.pop(0))\n",
    "# len(switch_list_1)\n",
    "\n",
    "# Not using since using Adj_Ret_Roll will likley overstate returns on the last day of expiry as they move quite dramatically on the last day\n",
    "# 2. Switching on the day of expiry\n",
    "switch_list_2 = [item for sublist in np.where([Lum_1.index == Lum_1.expirationDate],1,0).tolist() for item in sublist]\n",
    "# len(switch_list_2)\n",
    "\n",
    "# 3. Switch to 2nd month at the start of contract expiry month\n",
    "dt_lists = []\n",
    "for end_dt in Lum_1.expirationDate.unique():\n",
    "    end_dt = pd.to_datetime(end_dt)\n",
    "    for dt in pd.date_range(start=end_dt.strftime(\"%Y-%m-1\"), end=end_dt):\n",
    "        if dt in Lum_1.index:\n",
    "            dt_lists.append(dt)\n",
    "\n",
    "switch_list_3 = [1 if dt in dt_lists else 0 for dt in Lum_1.index]\n",
    "# len(switch_list_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculates Adjusted Returns, Basis and Basis by DaysToExpiry based on Switching Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>openInterest</th>\n",
       "      <th>symbol</th>\n",
       "      <th>expirationDate</th>\n",
       "      <th>Days_to_expiry</th>\n",
       "      <th>cash_price</th>\n",
       "      <th>Roll_Ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>open</th>\n",
       "      <th>Adj_open</th>\n",
       "      <th>Adj_high</th>\n",
       "      <th>Adj_low</th>\n",
       "      <th>Adj_close</th>\n",
       "      <th>Adj_ret</th>\n",
       "      <th>Adj_ret_roll</th>\n",
       "      <th>Basis</th>\n",
       "      <th>BasisByDte</th>\n",
       "      <th>First-Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-26</th>\n",
       "      <td>549.000000</td>\n",
       "      <td>511.799988</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LBU22</td>\n",
       "      <td>2022-09-15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>511.799988</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>0.006899</td>\n",
       "      <td>0.006899</td>\n",
       "      <td>26.400024</td>\n",
       "      <td>1.760002</td>\n",
       "      <td>28.599976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-25</th>\n",
       "      <td>529.900024</td>\n",
       "      <td>495.200012</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>182.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>LBU22</td>\n",
       "      <td>2022-09-15</td>\n",
       "      <td>16.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>495.399994</td>\n",
       "      <td>495.399994</td>\n",
       "      <td>529.900024</td>\n",
       "      <td>495.200012</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>0.048463</td>\n",
       "      <td>0.048463</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969-10-02</th>\n",
       "      <td>80.500000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LBH70</td>\n",
       "      <td>1970-03-13</td>\n",
       "      <td>117.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4084.353631</td>\n",
       "      <td>4109.880841</td>\n",
       "      <td>4058.826421</td>\n",
       "      <td>4109.880841</td>\n",
       "      <td>0.006231</td>\n",
       "      <td>0.006231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969-10-01</th>\n",
       "      <td>80.250000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LBH70</td>\n",
       "      <td>1970-03-13</td>\n",
       "      <td>118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4097.117236</td>\n",
       "      <td>3982.244790</td>\n",
       "      <td>4084.353631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  high         low       close  volume  openInterest symbol  \\\n",
       "2022-08-26  549.000000  511.799988  523.599976     NaN           NaN  LBU22   \n",
       "2022-08-25  529.900024  495.200012  520.000000   182.0         847.0  LBU22   \n",
       "1969-10-02   80.500000   79.500000   80.500000     NaN           NaN  LBH70   \n",
       "1969-10-01   80.250000   78.000000   80.000000     NaN           NaN  LBH70   \n",
       "\n",
       "           expirationDate  Days_to_expiry  cash_price  Roll_Ratio  ...  \\\n",
       "2022-08-26     2022-09-15            15.0       550.0         1.0  ...   \n",
       "2022-08-25     2022-09-15            16.0       550.0         1.0  ...   \n",
       "1969-10-02     1970-03-13           117.0         NaN         1.0  ...   \n",
       "1969-10-01     1970-03-13           118.0         NaN         1.0  ...   \n",
       "\n",
       "                  open     Adj_open     Adj_high      Adj_low    Adj_close  \\\n",
       "2022-08-26  520.000000   520.000000   549.000000   511.799988   523.599976   \n",
       "2022-08-25  495.399994   495.399994   529.900024   495.200012   520.000000   \n",
       "1969-10-02   80.000000  4084.353631  4109.880841  4058.826421  4109.880841   \n",
       "1969-10-01         NaN          NaN  4097.117236  3982.244790  4084.353631   \n",
       "\n",
       "             Adj_ret  Adj_ret_roll      Basis  BasisByDte  First-Second  \n",
       "2022-08-26  0.006899      0.006899  26.400024    1.760002     28.599976  \n",
       "2022-08-25  0.048463      0.048463  30.000000    1.875000     24.000000  \n",
       "1969-10-02  0.006231      0.006231        NaN         NaN     -0.750000  \n",
       "1969-10-01       NaN           NaN        NaN         NaN     -0.750000  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes 1 minute to run\n",
    "# Adj_Lum_1 = calculate_adj_ret(Lum_1,Lum_2,Lum_3, switch_list_1,switch_second=False) # Not using\n",
    "Adj_Lum_2 = calculate_adj_ret(Lum_1,Lum_2,Lum_3, switch_list_2,switch_second=True)\n",
    "Adj_Lum_3 = calculate_adj_ret(Lum_1,Lum_2,Lum_3, switch_list_3,switch_second=True)\n",
    "\n",
    "Adj_Lum_3.head(2).append(Adj_Lum_3.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DELETE\n",
    "# lumber_adj_close = pd.DataFrame(Lumber_Adjusted_Prices_MnthStart[new_col_name])\n",
    "# lumber_adj_close.columns = ['Adj_close']\n",
    "# lumber_adj_close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Front Month Lumber Unadjusted High, Low, Close Price, Volume, openInterest, cash_price -  Note CWP database doesn't have Open prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>openInterest</th>\n",
       "      <th>cash_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-26</th>\n",
       "      <td>549.00</td>\n",
       "      <td>511.799988</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969-10-01</th>\n",
       "      <td>80.25</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              high         low       close  volume  openInterest  cash_price\n",
       "2022-08-26  549.00  511.799988  523.599976     NaN           NaN       550.0\n",
       "1969-10-01   80.25   78.000000   80.000000     NaN           NaN         NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_ = ['high','low','close','volume','openInterest','cash_price']\n",
    "lumber_unadj = Lum_1[cols_]\n",
    "lumber_unadj.head(1).append(lumber_unadj.tail(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Dataframes - Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>openInterest</th>\n",
       "      <th>cash_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-26</th>\n",
       "      <td>549.00</td>\n",
       "      <td>511.799988</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969-10-01</th>\n",
       "      <td>80.25</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              high         low       close  volume  openInterest  cash_price\n",
       "2022-08-26  549.00  511.799988  523.599976     NaN           NaN       550.0\n",
       "1969-10-01   80.25   78.000000   80.000000     NaN           NaN         NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since other data points may be on a day when Lumber didn't trade we use all days - once we have concatenated and forward filled - we can use just trading days\n",
    "daily_dataframe= pd.DataFrame(index = pd.date_range(start=lumber_unadj.index.min(), end=lumber_unadj.index.max()),data = lumber_unadj )\n",
    "daily_dataframe = daily_dataframe[~daily_dataframe.index.duplicated(keep='first')]\n",
    "daily_dataframe.index = pd.DatetimeIndex(daily_dataframe.index)\n",
    "daily_dataframe.sort_index(inplace=True, ascending=False)\n",
    "daily_dataframe.head(1).append(daily_dataframe.tail(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lumber contango, backwardation - Data available only through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from CWP database & excel files::  33%|███▎      | 1/3 [00:02<00:05,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Contract:1 00:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from CWP database & excel files::  67%|██████▋   | 2/3 [00:05<00:02,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Contract:2 00:05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting data from CWP database & excel files:: 100%|██████████| 3/3 [00:08<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Contract:3 00:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1-2</th>\n",
       "      <th>2-3</th>\n",
       "      <th>1-3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-26</th>\n",
       "      <td>28.599976</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>5.599976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-07</th>\n",
       "      <td>-17.800000</td>\n",
       "      <td>-18.7</td>\n",
       "      <td>-36.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1-2   2-3        1-3\n",
       "date                                  \n",
       "2022-08-26  28.599976 -23.0   5.599976\n",
       "2011-01-07 -17.800000 -18.7 -36.500000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Live Close Price from CWP Database\n",
    "Lumber_LIVE = get_lumber_db_xl() \n",
    "Lumber = Lumber_LIVE.copy()\n",
    "front_month_price = Lumber[1]\n",
    "second_month_price = Lumber[2]\n",
    "third_month_price = Lumber[3]\n",
    "lumber_contract_spreads = pd.concat([front_month_price['close'], second_month_price['close'],third_month_price['close']],axis=1)\n",
    "lumber_contract_spreads.columns = ['front','second','third']\n",
    "lumber_contract_spreads['1-2'] = lumber_contract_spreads['front'] - lumber_contract_spreads['second']\n",
    "lumber_contract_spreads['2-3'] = lumber_contract_spreads['second'] - lumber_contract_spreads['third']\n",
    "lumber_contract_spreads['1-3'] = lumber_contract_spreads['front'] - lumber_contract_spreads['third']\n",
    "lumber_contract_spreads.drop(['front','second','third'],axis=1, inplace=True)\n",
    "lumber_contract_spreads.head(1).append(lumber_contract_spreads.tail(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lumber Moving Averages - USE TA lib for Technical Indicators\n",
    "#### For now subtracted each moving average from Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MA200</th>\n",
       "      <th>MA100</th>\n",
       "      <th>MA50</th>\n",
       "      <th>MA30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-26</th>\n",
       "      <td>274.940828</td>\n",
       "      <td>152.748147</td>\n",
       "      <td>72.551192</td>\n",
       "      <td>32.563356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-07-17</th>\n",
       "      <td>629.200622</td>\n",
       "      <td>286.753727</td>\n",
       "      <td>57.928411</td>\n",
       "      <td>-22.206839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MA200       MA100       MA50       MA30\n",
       "2022-08-26  274.940828  152.748147  72.551192  32.563356\n",
       "1970-07-17  629.200622  286.753727  57.928411 -22.206839"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lumber = Lumber_LIVE.copy()\n",
    "# close_price = lumber_unadj['close'].copy(deep = True).sort_index()\n",
    "close_price = Adj_Lum_3['Adj_close'].copy(deep = True).sort_index()\n",
    "MA200 = pd.DataFrame(close_price.rolling(200).mean().dropna().sort_index(ascending = False))\n",
    "MA100 = pd.DataFrame(close_price.rolling(100).mean().dropna().sort_index(ascending = False))\n",
    "MA50 = pd.DataFrame(close_price.rolling(50).mean().dropna().sort_index(ascending = False))\n",
    "MA30 = pd.DataFrame(close_price.rolling(30).mean().dropna().sort_index(ascending = False))\n",
    "MA1 = pd.DataFrame(close_price.rolling(1).mean().dropna().sort_index(ascending = False))\n",
    "\n",
    "# lumber_moving_averages = pd.concat([MA200['close'], MA100['close'],MA50['close'],MA30['close'],MA1['close']],axis=1)\n",
    "lumber_moving_averages = pd.concat([MA200['Adj_close'], MA100['Adj_close'],MA50['Adj_close'],MA30['Adj_close'],MA1['Adj_close']],axis=1)\n",
    "lumber_moving_averages.columns = ['MA200','MA100','MA50','MA30','Px']\n",
    "lumber_moving_averages = lumber_moving_averages.dropna().sort_index(ascending=False)\n",
    "\n",
    "# Subtracting each column from current price (Px)\n",
    "lumber_moving_averages.update(lumber_moving_averages[['MA200','MA100','MA50','MA30']].sub(lumber_moving_averages.Px, axis=0))\n",
    "lumber_moving_averages.drop(['Px'],axis=1, inplace=True)\n",
    "lumber_moving_averages.head(1).append(lumber_moving_averages.tail(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lumber Basis - Can get basis for other tickers as well\n",
    "\n",
    "Day of week;\tDay;\tReports     \n",
    "0;\tMon;\tLCBQ        \n",
    "1;\tTue;\tMAGQ  (Midweek Published after close)      \n",
    "2;\tWed;\tMAGQ        \n",
    "3;\tThurs;\tLCBQ  (Weekend report published after close)      \n",
    "4;\tFri;\tLCBQ        \n",
    "5;\tSat;\tLCBQ        \n",
    "6;\tSun;\tLCBQ        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Dataframes inserted into one - 3dfs represent 3 switching logic - No Switching (Not Using), Switch on Expiry day and Switch on start of Expiry Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open2</th>\n",
       "      <th>Adj_open2</th>\n",
       "      <th>high2</th>\n",
       "      <th>Adj_high2</th>\n",
       "      <th>low2</th>\n",
       "      <th>Adj_low2</th>\n",
       "      <th>close2</th>\n",
       "      <th>Adj_close2</th>\n",
       "      <th>Adj_ret_roll2</th>\n",
       "      <th>Basis2</th>\n",
       "      <th>...</th>\n",
       "      <th>Adj_low3</th>\n",
       "      <th>close3</th>\n",
       "      <th>Adj_close3</th>\n",
       "      <th>Adj_ret_roll3</th>\n",
       "      <th>Basis3</th>\n",
       "      <th>BasisByDte3</th>\n",
       "      <th>Days_to_expiry3</th>\n",
       "      <th>First-Second3</th>\n",
       "      <th>volume3</th>\n",
       "      <th>openInterest3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-26</th>\n",
       "      <td>520.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>511.799988</td>\n",
       "      <td>511.799988</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>0.006899</td>\n",
       "      <td>26.400024</td>\n",
       "      <td>...</td>\n",
       "      <td>511.799988</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>0.006899</td>\n",
       "      <td>26.400024</td>\n",
       "      <td>1.760002</td>\n",
       "      <td>15.0</td>\n",
       "      <td>28.599976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-25</th>\n",
       "      <td>495.399994</td>\n",
       "      <td>495.399994</td>\n",
       "      <td>529.900024</td>\n",
       "      <td>529.900024</td>\n",
       "      <td>495.200012</td>\n",
       "      <td>495.200012</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>0.048463</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>495.200012</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>0.048463</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>182.0</td>\n",
       "      <td>847.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969-10-02</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>16556.801851</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>16660.281862</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>16453.321839</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>16660.281862</td>\n",
       "      <td>0.006231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4058.826421</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>4109.880841</td>\n",
       "      <td>0.006231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117.0</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969-10-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.250000</td>\n",
       "      <td>16608.541857</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>16142.881805</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>16556.801851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3982.244790</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4084.353631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118.0</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 open2     Adj_open2       high2     Adj_high2        low2  \\\n",
       "2022-08-26  520.000000    520.000000  549.000000    549.000000  511.799988   \n",
       "2022-08-25  495.399994    495.399994  529.900024    529.900024  495.200012   \n",
       "1969-10-02   80.000000  16556.801851   80.500000  16660.281862   79.500000   \n",
       "1969-10-01         NaN           NaN   80.250000  16608.541857   78.000000   \n",
       "\n",
       "                Adj_low2      close2    Adj_close2  Adj_ret_roll2     Basis2  \\\n",
       "2022-08-26    511.799988  523.599976    523.599976       0.006899  26.400024   \n",
       "2022-08-25    495.200012  520.000000    520.000000       0.048463  30.000000   \n",
       "1969-10-02  16453.321839   80.500000  16660.281862       0.006231        NaN   \n",
       "1969-10-01  16142.881805   80.000000  16556.801851            NaN        NaN   \n",
       "\n",
       "            ...     Adj_low3      close3   Adj_close3  Adj_ret_roll3  \\\n",
       "2022-08-26  ...   511.799988  523.599976   523.599976       0.006899   \n",
       "2022-08-25  ...   495.200012  520.000000   520.000000       0.048463   \n",
       "1969-10-02  ...  4058.826421   80.500000  4109.880841       0.006231   \n",
       "1969-10-01  ...  3982.244790   80.000000  4084.353631            NaN   \n",
       "\n",
       "               Basis3  BasisByDte3  Days_to_expiry3  First-Second3  volume3  \\\n",
       "2022-08-26  26.400024     1.760002             15.0      28.599976      NaN   \n",
       "2022-08-25  30.000000     1.875000             16.0      24.000000    182.0   \n",
       "1969-10-02        NaN          NaN            117.0      -0.750000      NaN   \n",
       "1969-10-01        NaN          NaN            118.0      -0.750000      NaN   \n",
       "\n",
       "            openInterest3  \n",
       "2022-08-26            NaN  \n",
       "2022-08-25          847.0  \n",
       "1969-10-02            NaN  \n",
       "1969-10-01            NaN  \n",
       "\n",
       "[4 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Adj_ret_roll and not our calculated Adj_ret as described above\n",
    "cols_to_insert = ['open','Adj_open','high','Adj_high','low','Adj_low','close','Adj_close','Adj_ret_roll','Basis','BasisByDte','Days_to_expiry','First-Second','volume','openInterest']\n",
    "cols_to_insert_2 = []\n",
    "cols_to_insert_3 = []\n",
    "for col in cols_to_insert:\n",
    "    cols_to_insert_3.append(str(col)+'3')\n",
    "    cols_to_insert_2.append(str(col)+'2')\n",
    "df2 = Adj_Lum_2[cols_to_insert]\n",
    "df2.columns = cols_to_insert_2\n",
    "\n",
    "df3 = Adj_Lum_3[cols_to_insert]\n",
    "df3.columns = cols_to_insert_3\n",
    "\n",
    "adj_df = pd.concat([df2,df3], axis=1)\n",
    "adj_df.head(2).append(adj_df.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Investing.com - dictionary (Investing_dict) - \n",
    "1. US_MBA_Purchase_Index\n",
    "2. US_MBA_30_Yr_Mortgage_Rate\n",
    "3. US_Mortgage_Refinance_Index\n",
    "4. US_Mortgage_Market_Index\n",
    "5. US_MBA_Mortgage_Applications_WoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\source\\2x4-data\\app\\model_building\\Data_Script Updated.ipynb Cell 25\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/source/2x4-data/app/model_building/Data_Script%20Updated.ipynb#X33sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m req \u001b[39m=\u001b[39m Request(site, headers\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mMozilla/5.0\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/source/2x4-data/app/model_building/Data_Script%20Updated.ipynb#X33sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m html \u001b[39m=\u001b[39m urlopen(req)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/source/2x4-data/app/model_building/Data_Script%20Updated.ipynb#X33sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m bs \u001b[39m=\u001b[39m BeautifulSoup(html, \u001b[39m\"\u001b[39;49m\u001b[39mhtml5lib\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/source/2x4-data/app/model_building/Data_Script%20Updated.ipynb#X33sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m totals \u001b[39m=\u001b[39m [s\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m bs\u001b[39m.\u001b[39mfind_all(\u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/source/2x4-data/app/model_building/Data_Script%20Updated.ipynb#X33sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m first_table \u001b[39m=\u001b[39m bs\u001b[39m.\u001b[39mselect_one(\u001b[39m\"\u001b[39m\u001b[39mtable:nth-of-type(1)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\source\\2x4-data\\venv\\Lib\\site-packages\\bs4\\__init__.py:245\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    243\u001b[0m     builder_class \u001b[39m=\u001b[39m builder_registry\u001b[39m.\u001b[39mlookup(\u001b[39m*\u001b[39mfeatures)\n\u001b[0;32m    244\u001b[0m     \u001b[39mif\u001b[39;00m builder_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 245\u001b[0m         \u001b[39mraise\u001b[39;00m FeatureNotFound(\n\u001b[0;32m    246\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a tree builder with the features you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequested: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Do you need to install a parser library?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(features))\n\u001b[0;32m    250\u001b[0m \u001b[39m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[39m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[39m# with the remaining **kwargs.\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "Investing_dict = {}\n",
    "\n",
    "# 1. US_MBA_Purchase_Index\n",
    "# Arguments  to change\n",
    "path_mba = 'F:/Traders/2x4/2x4 v2/Data/Macro/US_MBA_Purchase_Index.xlsx'\n",
    "site= \"https://www.investing.com/economic-calendar/mba-purchase-index-1494\"\n",
    "columns_full = ['Time','Actual', 'Previous']\n",
    "cols_num = ['Actual', 'Previous']\n",
    "key_name = 'US_MBA_Purchase_Index'\n",
    "\n",
    "\n",
    "#Weekly Historical Data from excel\n",
    "\n",
    "mba  = pd.read_excel(path_mba, sheet_name='data',index_col=0)\n",
    "mba.index = pd.DatetimeIndex(mba.index).strftime('%Y-%m-%d')\n",
    "mba.index.name = 'dates_dt'\n",
    "mba = mba[columns_full]\n",
    "mba.head(3).append(mba.tail(3))\n",
    "\n",
    "\n",
    "## CURRENT DATA - Data from web\n",
    "req = Request(site, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "html = urlopen(req)\n",
    "bs = BeautifulSoup(html, \"html5lib\")\n",
    "totals = [s.encode('utf-8') for s in bs.find_all(\"table\")]\n",
    "first_table = bs.select_one(\"table:nth-of-type(1)\")\n",
    "rows = first_table.findAll('tr')\n",
    "data = [[td.findChildren(text=True) for td in tr.findAll(\"td\")] for tr in rows]\n",
    "df = pd.DataFrame.from_records(data)\n",
    "df = df.applymap(lambda x: x if not isinstance(x, list) else x[0] if len(x) else '')\n",
    "df = df.iloc[1: , :]\n",
    "\n",
    "df.drop(3, axis=1, inplace=True)\n",
    "df.drop(5, axis=1, inplace=True)\n",
    "df.index = pd.DatetimeIndex(df[0].to_list())\n",
    "df.drop([0], axis=1, inplace=True)\n",
    "df.columns = columns_full\n",
    "df[cols_num] = df[cols_num].apply(pd.to_numeric, errors = 'ignore')\n",
    "\n",
    "try:\n",
    "    for col in cols_num:\n",
    "        try: \n",
    "            df[col] = df[col].apply(lambda x: x.replace('%', ''))\n",
    "        except:\n",
    "            pass\n",
    "        df[col] = df[col].astype(float)\n",
    "        # if key_name in ['US_MBA_Mortgage_Applications_WoW','US_MBA_30_Yr_Mortgage_Rate']:\n",
    "        #     df[col] = df[col]/100\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i in df.index:\n",
    "    try:\n",
    "        df.loc[i:,'Actual']= df.loc[i:,'Actual'].astype(float)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Combined Data\n",
    "df = pd.concat([df,mba],axis=0, ignore_index=False)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "mba = df.copy()\n",
    "mba = mba.groupby(mba.index).first() # drop duplicates\n",
    "mba.sort_index(inplace=True,ascending = False)\n",
    "\n",
    "Investing_dict[key_name] = mba\n",
    "Investing_dict[key_name]\n",
    "\n",
    "# 2. US_MBA_30_Yr_Mortgage_Rate\n",
    "# Arguments  to change\n",
    "path_mba = 'F:/Traders/2x4/2x4 v2/Data/Macro/US_MBA_30_Yr_Mortgage_Rate.xlsx'\n",
    "site= \"https://www.investing.com/economic-calendar/mba-30-year-mortgage-rate-1042\"\n",
    "columns_full = ['Time','Actual', 'Previous']\n",
    "cols_num = ['Actual', 'Previous']\n",
    "key_name = 'US_MBA_30_Yr_Mortgage_Rate'\n",
    "\n",
    "#Weekly Historical Data from excel\n",
    "mba  = pd.read_excel(path_mba, sheet_name='data',index_col=0)\n",
    "mba.index = pd.DatetimeIndex(mba.index).strftime('%Y-%m-%d')\n",
    "mba.index.name = 'dates_dt'\n",
    "mba = mba[columns_full]\n",
    "mba.head(3).append(mba.tail(3))\n",
    "\n",
    "\n",
    "## CURRENT DATA - Data from web\n",
    "req = Request(site, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "html = urlopen(req)\n",
    "bs = BeautifulSoup(html, \"html5lib\")\n",
    "totals = [s.encode('utf-8') for s in bs.find_all(\"table\")]\n",
    "first_table = bs.select_one(\"table:nth-of-type(1)\")\n",
    "rows = first_table.findAll('tr')\n",
    "data = [[td.findChildren(text=True) for td in tr.findAll(\"td\")] for tr in rows]\n",
    "df = pd.DataFrame.from_records(data)\n",
    "df = df.applymap(lambda x: x if not isinstance(x, list) else x[0] if len(x) else '')\n",
    "df = df.iloc[1: , :]\n",
    "\n",
    "df.drop(3, axis=1, inplace=True)\n",
    "df.drop(5, axis=1, inplace=True)\n",
    "df.index = pd.DatetimeIndex(df[0].to_list())\n",
    "df.drop([0], axis=1, inplace=True)\n",
    "df.columns = columns_full\n",
    "df[cols_num] = df[cols_num].apply(pd.to_numeric, errors = 'ignore')\n",
    "\n",
    "try:\n",
    "    for col in cols_num:\n",
    "        try: \n",
    "            df[col] = df[col].apply(lambda x: x.replace('%', ''))\n",
    "        except:\n",
    "            pass\n",
    "        df[col] = df[col].astype(float)\n",
    "        # if key_name in ['US_MBA_Mortgage_Applications_WoW','US_MBA_30_Yr_Mortgage_Rate']:\n",
    "        #     df[col] = df[col]/100\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i in df.index:\n",
    "    try:\n",
    "        df.loc[i:,'Actual']= df.loc[i:,'Actual'].astype(float)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "df['Actual'][1:] = pd.to_numeric(df['Actual'][1:]).div(100)\n",
    "\n",
    "# Combined Data\n",
    "df = pd.concat([df,mba],axis=0, ignore_index=False)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "mba = df.copy()\n",
    "mba = mba.groupby(mba.index).first() # drop duplicates\n",
    "mba.sort_index(inplace=True,ascending = False)\n",
    "\n",
    "Investing_dict[key_name] = mba\n",
    "Investing_dict[key_name]\n",
    "\n",
    "\n",
    "# 3. US_Mortgage_Refinance_Index\n",
    "# Arguments  to change\n",
    "path_mba = 'F:/Traders/2x4/2x4 v2/Data/Macro/US_Mortgage Refinance Index.xlsx'\n",
    "site= \"https://www.investing.com/economic-calendar/mortgage-refinance-index-1428\"\n",
    "columns_full = ['Time','Actual', 'Previous']\n",
    "cols_num = ['Actual', 'Previous']\n",
    "key_name = 'US_Mortgage_Refinance_Index'\n",
    "\n",
    "\n",
    "#Weekly Historical Data from excel\n",
    "\n",
    "mba  = pd.read_excel(path_mba, sheet_name='data',index_col=0)\n",
    "mba.index = pd.DatetimeIndex(mba.index).strftime('%Y-%m-%d')\n",
    "mba.index.name = 'dates_dt'\n",
    "mba = mba[columns_full]\n",
    "mba.head(3).append(mba.tail(3))\n",
    "\n",
    "\n",
    "## CURRENT DATA - Data from web\n",
    "req = Request(site, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "html = urlopen(req)\n",
    "bs = BeautifulSoup(html, \"html5lib\")\n",
    "totals = [s.encode('utf-8') for s in bs.find_all(\"table\")]\n",
    "first_table = bs.select_one(\"table:nth-of-type(1)\")\n",
    "rows = first_table.findAll('tr')\n",
    "data = [[td.findChildren(text=True) for td in tr.findAll(\"td\")] for tr in rows]\n",
    "df = pd.DataFrame.from_records(data)\n",
    "df = df.applymap(lambda x: x if not isinstance(x, list) else x[0] if len(x) else '')\n",
    "df = df.iloc[1: , :]\n",
    "\n",
    "df.drop(3, axis=1, inplace=True)\n",
    "df.drop(5, axis=1, inplace=True)\n",
    "df.index = pd.DatetimeIndex(df[0].to_list())\n",
    "df.drop([0], axis=1, inplace=True)\n",
    "df.columns = columns_full\n",
    "df[cols_num] = df[cols_num].apply(pd.to_numeric, errors = 'ignore')\n",
    "\n",
    "try:\n",
    "    for col in cols_num:\n",
    "        try: \n",
    "            df[col] = df[col].apply(lambda x: x.replace('%', ''))\n",
    "        except:\n",
    "            pass\n",
    "        df[col] = df[col].astype(float)\n",
    "        # if key_name in ['US_MBA_Mortgage_Applications_WoW','US_MBA_30_Yr_Mortgage_Rate']:\n",
    "        #     df[col] = df[col]/100\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i in df.index:\n",
    "    try:\n",
    "        df.loc[i:,'Actual']= df.loc[i:,'Actual'].astype(float)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Combined Data\n",
    "df = pd.concat([df,mba],axis=0, ignore_index=False)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "mba = df.copy()\n",
    "mba = mba.groupby(mba.index).first() # drop duplicates\n",
    "mba.sort_index(inplace=True,ascending = False)\n",
    "\n",
    "Investing_dict[key_name] = mba\n",
    "Investing_dict[key_name]\n",
    "\n",
    "\n",
    "# 4. US_Mortgage_Market_Index\n",
    "# Arguments  to change\n",
    "path_mba = 'F:/Traders/2x4/2x4 v2/Data/Macro/US_Mortgage_Market_Index.xlsx'\n",
    "site= \"https://www.investing.com/economic-calendar/mortgage-market-index-1427\"\n",
    "columns_full = ['Time','Actual', 'Previous']\n",
    "cols_num = ['Actual', 'Previous']\n",
    "key_name = 'US_Mortgage_Market_Index'\n",
    "\n",
    "\n",
    "#Weekly Historical Data from excel\n",
    "mba  = pd.read_excel(path_mba, sheet_name='data',index_col=0)\n",
    "mba.index = pd.DatetimeIndex(mba.index).strftime('%Y-%m-%d')\n",
    "mba.index.name = 'dates_dt'\n",
    "mba = mba[columns_full]\n",
    "mba.head(3).append(mba.tail(3))\n",
    "\n",
    "\n",
    "## CURRENT DATA - Data from web\n",
    "req = Request(site, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "html = urlopen(req)\n",
    "bs = BeautifulSoup(html, \"html5lib\")\n",
    "totals = [s.encode('utf-8') for s in bs.find_all(\"table\")]\n",
    "first_table = bs.select_one(\"table:nth-of-type(1)\")\n",
    "rows = first_table.findAll('tr')\n",
    "data = [[td.findChildren(text=True) for td in tr.findAll(\"td\")] for tr in rows]\n",
    "df = pd.DataFrame.from_records(data)\n",
    "df = df.applymap(lambda x: x if not isinstance(x, list) else x[0] if len(x) else '')\n",
    "df = df.iloc[1: , :]\n",
    "\n",
    "df.drop(3, axis=1, inplace=True)\n",
    "df.drop(5, axis=1, inplace=True)\n",
    "df.index = pd.DatetimeIndex(df[0].to_list())\n",
    "df.drop([0], axis=1, inplace=True)\n",
    "df.columns = columns_full\n",
    "df[cols_num] = df[cols_num].apply(pd.to_numeric, errors = 'ignore')\n",
    "\n",
    "try:\n",
    "    for col in cols_num:\n",
    "        try: \n",
    "            df[col] = df[col].apply(lambda x: x.replace('%', ''))\n",
    "        except:\n",
    "            pass\n",
    "        df[col] = df[col].astype(float)\n",
    "        # if key_name in ['US_MBA_Mortgage_Applications_WoW','US_MBA_30_Yr_Mortgage_Rate']:\n",
    "        #     df[col] = df[col]/100\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i in df.index:\n",
    "    try:\n",
    "        df.loc[i:,'Actual']= df.loc[i:,'Actual'].astype(float)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Combined Data\n",
    "df = pd.concat([df,mba],axis=0, ignore_index=False)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "mba = df.copy()\n",
    "mba = mba.groupby(mba.index).first() # drop duplicates\n",
    "mba.sort_index(inplace=True,ascending = False)\n",
    "\n",
    "Investing_dict[key_name] = mba\n",
    "Investing_dict[key_name]\n",
    "\n",
    "\n",
    "# 5. US_MBA_Mortgage_Applications_WoW\n",
    "# Arguments  to change\n",
    "path_mba = 'F:/Traders/2x4/2x4 v2/Data/Macro/US_MBA_Mortgage_Applications_WoW.xlsx'\n",
    "site= \"https://www.investing.com/economic-calendar/mba-mortgage-applications-380\"\n",
    "columns_full = ['Time','Actual', 'Previous']\n",
    "cols_num = ['Actual', 'Previous']\n",
    "key_name = 'US_MBA_Mortgage_Applications_WoW'\n",
    "\n",
    "\n",
    "#Weekly Historical Data from excel\n",
    "\n",
    "mba  = pd.read_excel(path_mba, sheet_name='data',index_col=0)\n",
    "mba.index = pd.DatetimeIndex(mba.index).strftime('%Y-%m-%d')\n",
    "mba.index.name = 'dates_dt'\n",
    "mba = mba[columns_full]\n",
    "mba.head(3).append(mba.tail(3))\n",
    "\n",
    "\n",
    "## CURRENT DATA - Data from web\n",
    "req = Request(site, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "html = urlopen(req)\n",
    "bs = BeautifulSoup(html, \"html5lib\")\n",
    "totals = [s.encode('utf-8') for s in bs.find_all(\"table\")]\n",
    "first_table = bs.select_one(\"table:nth-of-type(1)\")\n",
    "rows = first_table.findAll('tr')\n",
    "data = [[td.findChildren(text=True) for td in tr.findAll(\"td\")] for tr in rows]\n",
    "df = pd.DataFrame.from_records(data)\n",
    "df = df.applymap(lambda x: x if not isinstance(x, list) else x[0] if len(x) else '')\n",
    "df = df.iloc[1: , :]\n",
    "\n",
    "df.drop(3, axis=1, inplace=True)\n",
    "df.drop(5, axis=1, inplace=True)\n",
    "df.index = pd.DatetimeIndex(df[0].to_list())\n",
    "df.drop([0], axis=1, inplace=True)\n",
    "df.columns = columns_full\n",
    "df[cols_num] = df[cols_num].apply(pd.to_numeric, errors = 'ignore')\n",
    "\n",
    "try:\n",
    "    for col in cols_num:\n",
    "        try: \n",
    "            df[col] = df[col].apply(lambda x: x.replace('%', ''))\n",
    "        except:\n",
    "            pass\n",
    "        df[col] = df[col].astype(float)\n",
    "        # if key_name in ['US_MBA_Mortgage_Applications_WoW','US_MBA_30_Yr_Mortgage_Rate']:\n",
    "        #     df[col] = df[col]/100\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i in df.index:\n",
    "    try:\n",
    "        df.loc[i:,'Actual']= df.loc[i:,'Actual'].astype(float)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df['Actual'][1:] = pd.to_numeric(df['Actual'][1:]).div(100)\n",
    "\n",
    "# Combined Data\n",
    "df = pd.concat([df,mba],axis=0, ignore_index=False)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "mba = df.copy()\n",
    "mba = mba.groupby(mba.index).first() # drop duplicates\n",
    "mba.sort_index(inplace=True,ascending = False)\n",
    "\n",
    "Investing_dict[key_name] = mba\n",
    "\n",
    "# removing data for which Actual value is not present\n",
    "for keys in Investing_dict.keys():\n",
    "    Investing_dict[keys].dropna(inplace=True)\n",
    "\n",
    "# for keys in Investing_dict.keys():\n",
    "#     print(keys)\n",
    "#     print(\"Start\",Investing_dict[keys].index[0].strftime('%Y-%m-%d'))\n",
    "#     print(\"End\",Investing_dict[keys].index[-1].strftime('%Y-%m-%d'))\n",
    "\n",
    "Investing_dict.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAHB/Wells Fargo Housing Market Index (HMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_hmi_latest_url():\n",
    "    for i in range(0,12):\n",
    "        mnth = '{:02d}'.format((datetime.now() - pd.DateOffset(months=i)).month)\n",
    "        year = '{:02d}'.format((datetime.now() - pd.DateOffset(months=i)).year)\n",
    "        date_str = str(year)+str(mnth)      \n",
    "        try:\n",
    "            wells_hmi = str('https://www.nahb.org/-/media/NAHB/news-and-economics/docs/housing-economics/hmi/'+date_str+ '/t2-national-hmi-history-'+date_str+'.xlsx')\n",
    "            wells_hmi_df = pd.read_excel(wells_hmi, sheet_name='Table2 HMI(History)', skiprows=2)\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "    return wells_hmi\n",
    "\n",
    "wells_hmi = get_hmi_latest_url()\n",
    "wells_hmi_df = pd.read_excel(wells_hmi, sheet_name='Table2 HMI(History)', skiprows=2)\n",
    "wells_hmi_df.index = wells_hmi_df['Unnamed: 0']\n",
    "wells_hmi_df = wells_hmi_df.drop(columns=['Unnamed: 0'])\n",
    "wells_hmi_df.index.name = 'Year'\n",
    "wells_hmi_df = wells_hmi_df.unstack()\n",
    "wells_hmi_df.index = pd.to_datetime([f'{y}-{m}-01' for m, y in wells_hmi_df.index])\n",
    "wells_hmi_df = pd.DataFrame(wells_hmi_df.values, index=wells_hmi_df.index, columns=['HMI'])\n",
    "wells_hmi_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OverNight Funding Rates - NEWYORK FED     \n",
    "\n",
    "- Rates Dict (overnight_rates) and Averagerate dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "enddates_dt = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# Overnight Unsecured rates (EFFR, OBFR, TGCR, BGCR, SOFR)\n",
    "\n",
    "link_first = \"https://markets.newyorkfed.org/read?startDt=2000-01-01&endDt=\"\n",
    "link_last = \"&eventCodes=510,515,520,500,505&productCode=50&sort=postdates_dt:-1,eventCode:1&format=xlsx\"\n",
    "# overnight_rates = 'https://markets.newyorkfed.org/read?startdates_dt=2000-01-01&enddates_dt=2022-05-24&eventCodes=510,515,520,500,505&productCode=50&sort=postdates_dt:-1,eventCode:1&format=xlsx'\n",
    "overnight_rates = link_first + enddates_dt + link_last\n",
    "warnings.filterwarnings('ignore')\n",
    "overnight_rates = pd.read_excel(overnight_rates)\n",
    "overnight_rates.set_index('Effective Date', inplace=True)\n",
    "\n",
    "# average_rates =\"https://markets.newyorkfed.org/read?startdates_dt=2000-01-01&enddates_dt=2022-05-24&eventCodes=525&productCode=50&sort=postdates_dt:-1,eventCode:1&format=xlsx\"\n",
    "average_rate_link_first = \"https://markets.newyorkfed.org/read?startDt=2000-01-01&endDt=\"\n",
    "average_rate_link_last = \"&eventCodes=525&productCode=50&sort=postdates_dt:-1,eventCode:1&format=xlsx\"\n",
    "average_rates = average_rate_link_first + enddates_dt + average_rate_link_last\n",
    "average_rates = pd.read_excel(average_rates)\n",
    "average_rates.set_index('Effective Date', inplace=True)\n",
    "average_rates = pd.DataFrame(average_rates['SOFR Index'])\n",
    "average_rates.index = pd.DatetimeIndex(average_rates.index).strftime('%Y-%m-%d')\n",
    "average_rates.index = pd.DatetimeIndex(average_rates.index)\n",
    "\n",
    "rates_dict = {}\n",
    "for rate in overnight_rates['Rate Type'].unique():\n",
    "    rates_dict[rate] = pd.DataFrame(overnight_rates[overnight_rates['Rate Type'] == rate].drop(['Rate Type'], axis=1)['Rate (%)'])\n",
    "    rates_dict[rate].index = pd.DatetimeIndex(rates_dict[rate].index).strftime('%Y-%m-%d')\n",
    "\n",
    "overnight_rates.index = pd.DatetimeIndex(overnight_rates.index)\n",
    "rates_df = overnight_rates.pivot_table(index='Effective Date', columns='Rate Type', values='Rate (%)')\n",
    "rates_df.sort_index(ascending=False, inplace=True)\n",
    "\n",
    "over_night_rates_df = pd.concat([rates_df,average_rates], axis=1)\n",
    "over_night_rates_df.sort_index(ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realtor Data https://www.realtor.com/research/data/ & GSCPI & FTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Weekly Realtor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_realtor_link = 'https://econdata.s3-us-west-2.amazonaws.com/Reports/Core/listing_weekly_core_aggregate_by_country.csv'\n",
    "weekly_realtor = pd.read_csv(weekly_realtor_link)\n",
    "weekly_realtor.index = pd.DatetimeIndex(weekly_realtor['week_end_date'])\n",
    "weekly_realtor.drop(['week_end_date'], axis=1, inplace=True)\n",
    "weekly_realtor.drop(['geo_country'], axis=1, inplace=True)\n",
    "weekly_realtor.index.name = 'dates_dt'\n",
    "\n",
    "# Convert strings to numeric values and remove % from cells\n",
    "for col in weekly_realtor.columns:\n",
    "    try:\n",
    "        weekly_realtor[col] = weekly_realtor[col].apply(lambda x: x.replace('%',''))\n",
    "        weekly_realtor[col] = pd.to_numeric(weekly_realtor[col], errors='coerce')\n",
    "    except:\n",
    "        weekly_realtor[col] = pd.to_numeric(weekly_realtor[col], errors='coerce')\n",
    "\n",
    "# Add % to column headers\n",
    "weekly_realtor.columns = ['{}{}'.format(c, '' if c in ['median_days_on_market_by_day_yy'] else ' (%)') for c in weekly_realtor.columns]\n",
    "\n",
    "weekly_realtor.head(1).append(weekly_realtor.tail(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Realtor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_monthly_realtor_link = 'https://econdata.s3-us-west-2.amazonaws.com/Reports/Core/RDC_Inventory_Core_Metrics_Country.csv'\n",
    "history_monthly_realtor_link = 'https://econdata.s3-us-west-2.amazonaws.com/Reports/Core/RDC_Inventory_Core_Metrics_Country_History.csv'\n",
    "\n",
    "monthly_realtor = pd.concat([pd.read_csv(current_monthly_realtor_link,on_bad_lines='skip'),pd.read_csv(history_monthly_realtor_link,on_bad_lines='skip')],axis=0)\n",
    "monthly_realtor.drop_duplicates(inplace=True)\n",
    "monthly_realtor.index = pd.to_datetime(monthly_realtor['month_date_yyyymm'], format='%Y%m', errors='coerce')\n",
    "monthly_realtor.drop(['month_date_yyyymm'], axis=1, inplace=True)\n",
    "monthly_realtor.index.name = 'dates_dt'\n",
    "monthly_realtor = monthly_realtor[monthly_realtor.index.notnull()]\n",
    "monthly_realtor.drop(['country','quality_flag'], axis=1, inplace=True)\n",
    "monthly_realtor.sort_index(ascending=False, inplace=True)\n",
    "\n",
    "monthly_realtor.head(1).append(monthly_realtor.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSCPI Index - from newyorkfed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSCPI_link = 'https://www.newyorkfed.org/medialibrary/research/interactives/gscpi/downloads/gscpi_data.xlsx'\n",
    "GSCPI = pd.read_excel(GSCPI_link, sheet_name='GSCPI Monthly Data',skiprows=4)\n",
    "GSCPI.set_index('Unnamed: 0', inplace=True)\n",
    "GSCPI.index = pd.DatetimeIndex(GSCPI.index)\n",
    "GSCPI.index.name = 'Date'\n",
    "GSCPI = GSCPI.drop(['Unnamed: 2','Unnamed: 3'], axis=1)\n",
    "GSCPI.columns = ['GSCPI']\n",
    "GSCPI.sort_index(ascending=False, inplace=True)\n",
    "GSCPI.head(1).append(GSCPI.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTR Historical Data - ETF from personal email - Need to somehow pull it automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receive on late 30th so delay by 4 days? - Receive on Monday for Week ending on Friday - But This is Daily Data\n",
    "ftr_path = 'F:/Traders/2x4/2x4 v2/Data/ETF/FTR Historical Data.xlsx'\n",
    "FTR = pd.read_excel(ftr_path, sheet_name='Forisk Timber REIT (FTR) Index',skiprows=6, index_col=1)\n",
    "FTR.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "FTR.index = pd.DatetimeIndex(FTR.index)\n",
    "FTR.sort_index(ascending=False, inplace=True)\n",
    "FTR.head(1).append(FTR.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Economic Index (WEI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# May 26th Data will come out on 2nd June & I think data comes out at 11:30 am - so lag by 7 calendar or 5 business days https://www.newyorkfed.org/research/policy/weekly-economic-index#/interactive\n",
    "\n",
    "WEI_LINK = \"https://www.newyorkfed.org/medialibrary/research/interactives/wei/downloads/weekly-economic-index_data.xlsx\"\n",
    "WEI = pd.read_excel(WEI_LINK, sheet_name='2008-current',skiprows=4)\n",
    "WEI.set_index('Date', inplace=True)\n",
    "WEI.index = pd.DatetimeIndex(WEI.index)\n",
    "WEI = WEI.iloc[: , :1] # Keep only 1st column\n",
    "WEI.sort_index(ascending=False, inplace=True)\n",
    "WEI.head(1).append(WEI.tail(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Yield Curve https://www.newyorkfed.org/research/capital_markets/ycfaq#/interactive\n",
    "#### MONTHLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is similar to GSCPI - COPYING FROM NOTES ABOVE - # \"We update the GSCPI at 10:00 a.m. on the fourth business day of each month.\"\" For example, April data is updated on the fourth business day of May. Also since the data is reported with end of month date, we simply do forward fill and will later put a lag of 4 (+1 - for last day of month) business days\n",
    "Y_curve_link = \"https://www.newyorkfed.org/medialibrary/media/research/capital_markets/allmonth.xls\"\n",
    "Y_curve = pd.read_excel(Y_curve_link, sheet_name='rec_prob', index_col=0)\n",
    "Y_curve = Y_curve.iloc[:, :4]\n",
    "Y_curve.index = pd.DatetimeIndex(Y_curve.index)\n",
    "Y_curve.dropna(inplace=True)\n",
    "Y_curve.sort_index(ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treasury Term Premia https://www.newyorkfed.org/research/data_indicators/term-premia-tabs#/interactive\n",
    "### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Treasury_Terms_link = \"https://www.newyorkfed.org/medialibrary/media/research/data_indicators/ACMTermPremium.xls\"\n",
    "Treasury_Terms = pd.read_excel(Treasury_Terms_link, sheet_name='ACM Daily', index_col=0)\n",
    "Treasury_Terms.index = pd.DatetimeIndex(Treasury_Terms.index)\n",
    "Treasury_Terms.dropna(inplace=True)\n",
    "Treasury_Terms.sort_index(ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _fred import FRED_Client, FRED_SYMBOLS\n",
    "fred = FRED_Client(FRED_SYMBOLS)\n",
    "fred_data = fred.get_data(100)\n",
    "fred_desc = fred.get_symbol_description()\n",
    "\n",
    "# All Dataseries are first segregated based on their frequency such as Monthly vs Daily etc and then based on that inserted in a dataframe.\n",
    "# The final output is a dictionary of dataframes - Monthly Dataframes, Weekly Dataframes, Daily Dataframes etc.\n",
    "\n",
    "import collections\n",
    "fred_daily_syms = []\n",
    "error_syms = []\n",
    "\n",
    "frequency_count = fred_desc.loc[['id','frequency']].T.groupby('frequency').count()\n",
    "fred_frquency_sym = collections.defaultdict(list)\n",
    "\n",
    "for key in frequency_count.index:\n",
    "    # print(key)\n",
    "    for syms in fred_data.keys():\n",
    "        # print(syms)\n",
    "        try:\n",
    "            if(fred_desc[syms]['frequency'] == key):\n",
    "                fred_frquency_sym[key].append(syms)\n",
    "        except:\n",
    "            error_syms.append(syms)\n",
    "\n",
    "fred_db_tables = collections.defaultdict(list)\n",
    "for key in fred_frquency_sym.keys():\n",
    "    fred_db_tables[key] =pd.DataFrame()\n",
    "    for sym in fred_frquency_sym[key]:\n",
    "        fred_db_tables[key] = pd.concat([fred_db_tables[key],fred_data[sym]],axis=1)\n",
    "    \n",
    "    fred_db_tables[key].index = pd.DatetimeIndex(fred_db_tables[key].index).strftime('%Y-%m-%d')\n",
    "    fred_db_tables[key] = fred_db_tables[key].dropna(how='all').sort_index(ascending = False)\n",
    "\n",
    "# NO Need to manually remove any series since     \n",
    "# fred_db_tables['Daily'].drop('TEDRATE',axis=1, inplace = True) # Removed TEDRATE from symbols with daily frequency as its 'last_updated' field in fred_desc was not updated for a long time\n",
    "\n",
    "\n",
    "## Rename All columns to original Names\n",
    "for key in fred_db_tables.keys():\n",
    "    col_list = []\n",
    "    for col in fred_db_tables[key].columns:\n",
    "        col_list.append(fred_desc.loc[['id','title']][col]['title'])\n",
    "    fred_db_tables[key].columns = col_list\n",
    "    fred_db_tables[key].index = pd.DatetimeIndex(fred_db_tables[key].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_count = fred_desc.loc[['id','frequency']].T.groupby('frequency').count()\n",
    "frequency_count.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAHOO - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _yahoo import YAHOO_Client, YAHOO_SYMBOLS\n",
    "yahoo = YAHOO_Client(YAHOO_SYMBOLS)\n",
    "data_close_yahoo = yahoo.get_combined_yahoo(OHLCV = 'Close') # This is Not for the Database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CN carloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _railroad import *\n",
    "# Remember the dates are for the week starting - CN Data starts on Sunday and for the next week\n",
    "cn_carloads = combined_company_railroad_data('CN',20)\n",
    "cn_carloads['Total Forest Products'] = cn_carloads['Primary Forest Prods'] + cn_carloads['Lumber & Wood Prods'] + cn_carloads['Pulp & Paper Prods']\n",
    "cn_carloads.index =  pd.DatetimeIndex(cn_carloads.index)\n",
    "cn_carloads.sort_index(ascending = False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CP carloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _railroad import *\n",
    "# Takes ~4 mins to run\n",
    "cp_carloads = combined_company_railroad_data('CP',8)\n",
    "cp_carloads['Total Forest Products'] = cp_carloads['Primary Forest Products'] + cp_carloads['Lumber & Wood Except Furniture'] + cp_carloads['Pulp,Paper & Allied Products']\n",
    "cp_carloads.index = pd.to_datetime(cp_carloads.index)\n",
    "cp_carloads.sort_index(ascending = False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CP Train Speed and CP Terminal Dwell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_train_speed = cp_railroad_TrainSpeed_xl()\n",
    "cp_train_speed.index = pd.to_datetime(cp_train_speed.index)\n",
    "cp_train_speed.sort_index(ascending = False, inplace=True)\n",
    "\n",
    "cp_terminal_dwell = cp_railroad_TerminalDwell_xl()\n",
    "cp_terminal_dwell.index = pd.to_datetime(cp_terminal_dwell.index)\n",
    "cp_terminal_dwell.sort_index(ascending = False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Census Monthly Reports\n",
    "1. Monthly Wholesale Trade: Sales and Inventories\n",
    "2. New Residential Construction\n",
    "3. New Home Sales\n",
    "4. Construction Spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _censusUS import *\n",
    "new_residential_construction = CensusEconomicIndicators(category_codes_reversed_construction_spending, data_type_codes_reversed_construction_spending, filter_list_construction_spending, time_text, params, 'New Residential Construction')\n",
    "new_residential_construction_df = new_residential_construction.aggregate_timeseries_without_labels()\n",
    "new_residential_construction_to_keep = [ x for x in list(new_residential_construction_df.columns) if \"_E_\" not in x ]\n",
    "new_residential_construction_df = new_residential_construction_df[new_residential_construction_to_keep]\n",
    "new_residential_construction_df.index = pd.DatetimeIndex(new_residential_construction_df.index)\n",
    "\n",
    "construct_spend = CensusEconomicIndicators(category_codes_reversed_construction_spending, data_type_codes_reversed_construction_spending, filter_list_construction_spending, time_text, params, 'Construction Spending')\n",
    "construct_spend_df = construct_spend.aggregate_timeseries()\n",
    "construct_spend_df.index = pd.DatetimeIndex(construct_spend_df.index)\n",
    "\n",
    "new_home_sales = CensusEconomicIndicators(category_codes_reversed_construction_spending, data_type_codes_reversed_construction_spending, filter_list_construction_spending, time_text, params, 'New Home Sales')\n",
    "new_home_sales_df = new_home_sales.aggregate_timeseries_without_labels()\n",
    "new_home_sales_to_keep = [ x for x in list(new_home_sales_df.columns) if \"_E_\" not in x ]\n",
    "new_home_sales_df = new_home_sales_df[new_home_sales_to_keep]\n",
    "new_home_sales_df.index  =  pd.DatetimeIndex(new_home_sales_df.index)\n",
    "\n",
    "monthly_wholesale_trade = CensusEconomicIndicators(category_codes_reversed_construction_spending, data_type_codes_reversed_construction_spending, filter_list_construction_spending, time_text, params, 'Monthly Wholesale Trade: Sales and Inventories')\n",
    "monthly_wholesale_trade_df = monthly_wholesale_trade.aggregate_timeseries_without_labels()\n",
    "monthly_wholesale_trade_to_keep = [ x for x in list(monthly_wholesale_trade_df.columns) if \"_E_\" not in x ]\n",
    "monthly_wholesale_trade_df = monthly_wholesale_trade_df[monthly_wholesale_trade_to_keep]\n",
    "codes_to_include = ['4232_IM','4232_SM','4233_IM','4233_SM']\n",
    "monthly_wholesale_trade_to_keep = [ x for x in list(monthly_wholesale_trade_df.columns) if x in codes_to_include ]\n",
    "monthly_wholesale_trade_df = monthly_wholesale_trade_df[monthly_wholesale_trade_to_keep]\n",
    "monthly_wholesale_trade_df.columns =  ['Furniture_Inventories_NSA','Furniture_Sales_NSA','Lumber_Inventories_NSA','Lumber_Sales_NSA'] \n",
    "monthly_wholesale_trade_df = monthly_wholesale_trade_df.apply(pd.to_numeric)\n",
    "monthly_wholesale_trade_df.index = pd.to_datetime(monthly_wholesale_trade_df.index).strftime('%Y-%m-%d')\n",
    "monthly_wholesale_trade_df['Furniture_Sales_By_Inv'] = monthly_wholesale_trade_df['Furniture_Inventories_NSA'].div(monthly_wholesale_trade_df['Furniture_Sales_NSA'])\n",
    "monthly_wholesale_trade_df['Lumber_Sales_By_Inv'] = monthly_wholesale_trade_df['Lumber_Inventories_NSA'].div(monthly_wholesale_trade_df['Lumber_Sales_NSA'])\n",
    "monthly_wholesale_trade_df.index = pd.DatetimeIndex(monthly_wholesale_trade_df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interest Rates & Inflation - pierian\n",
    "- To Add to daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pieriantraining.com/visualizing-historical-yield-curves-with-plotly-and-python/\n",
    "from _pierian import *\n",
    "interest_rate_pierian_df = get_interest_rates_pierian(start_yr=2002, end_yr=2023)\n",
    "interest_rate_pierian_df.head(1).append(interest_rate_pierian_df.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WWPA\n",
    "Lumber Track            \n",
    "Western Lumber Facts            \n",
    "Barometer- Weekly           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WesternWoodPDFs.westernpdfabstractor.backend.db.models import *\n",
    "wwpa_database_url=\"postgresql://fanjum:3cUDV4jctsfF0pYcxoMQ@pg-prod-master.data-191214.com:5432/lumber-data-sources\"\n",
    "wwpa_schema='wwpa'\n",
    "wwpa_ssn=connect_to_database(wwpa_database_url,wwpa_schema)\n",
    "\n",
    "List_of_models = [ProductionUS,ShipmentUS,InventoryUS,OrdersUS,PPPC,UnfilledOrdersUS,ProductionCanada,ShipmentCanada,\n",
    "                 InventoryCanada,LumberExportCanada,LogExportUS,LogImportsUS,ConsumptionLumberUs,LumberExportUS,\n",
    "                 LumberImportsUS,ConsumptionLumberCanada,NorthAmericanProduction,NorthAmericanShipment,\n",
    "                 NorthAmericanOrder,NorthAmericanUnfiledOrder,NorthAmericanInventory,OrdersWestern,PPPCWestern,\n",
    "                 ProductionWestern,BarometerWestern,BarometerCoast,BarometerInland,BarometerFinishedInventories,\n",
    "                 UnfilledOrderWestern,ShipmentWestern,InventoryWestern,ShipmentCoastal,ShipmentInland,AveragePriceCoastal,\n",
    "                 AveragePriceIsland,AveragePriceCostalDoglas,AveragePriceCostalHamfir,AveragePriceIslandDouglas,\n",
    "                 AveragePricePonderasoPine,AveragePriceWhiteWoods]\n",
    "\n",
    "# Thes 2 functions required to extract the name of the report from the update table\n",
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def get_report_name(link):\n",
    "    link_start = find_nth(link, \"\\\\\",8)\n",
    "    link_end = find_nth(link, \"\\\\\",9)\n",
    "    link_start +=1\n",
    "    name = link[link_start:link_end]\n",
    "    return name\n",
    "\n",
    "# From table name (refer to models column in update dataframe above), get class model name which will be used to fetch information from the database\n",
    "# from WesternWoodPDFs.westernpdfabstractor.backend.db.models import Base\n",
    "def get_class_by_tablename(table_fullname):\n",
    "  Base.TBLNAME_TO_CLASS = {}\n",
    "  for mapper in Base.registry.mappers:\n",
    "      cls = mapper.class_\n",
    "      classname = cls.__name__\n",
    "      if not classname.startswith('_'):\n",
    "          tblname = cls.__tablename__\n",
    "          Base.TBLNAME_TO_CLASS[tblname] = cls\n",
    "  return Base.TBLNAME_TO_CLASS[table_fullname]\n",
    "\n",
    "update_df  = pd.DataFrame(columns=['model','last_updated','Columns','report_name'])\n",
    "\n",
    "# to_delete_list = []\n",
    "\n",
    "List_of_error_models=[]\n",
    "List_of_empty_models=[]\n",
    "\n",
    "for model in List_of_models:\n",
    "    \n",
    "    try:\n",
    "        data=query_data(wwpa_ssn,model)\n",
    "        model_data = pd.DataFrame(data)\n",
    "        max_date = model_data['Timestamp'].max().strftime('%Y-%m-%d')\n",
    "        columns = model.__table__.columns.keys()\n",
    "        # .remove('Timestamp') Remove Timestamp from columns list\n",
    "        model_name = model.__tablename__\n",
    "        # to_delete_list.append(model_data['File'][0])\n",
    "        report_name = get_report_name(model_data['File'][0])\n",
    "        row_data = pd.DataFrame([[model_name,max_date,columns, report_name]],columns=['model','last_updated','Columns','report_name'])\n",
    "        update_df = pd.concat([update_df, row_data], axis=0)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        List_of_error_models.append(model.__tablename__)\n",
    "        print(\"error in model {}\".format(model.__tablename__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BARCHART Futures Data - Transformed & adjusted - NOT Using for now since its historical anyway not LIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 1 minute to run\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "# # CODE TO GET ALL DATA FROM csv files\n",
    "# col_name = 'Adj_close'\n",
    "# futures_df = pd.DataFrame(index=lumber_adj_close.index)\n",
    "# for f in files:\n",
    "#     full_name = str(path+'/'+f)    \n",
    "#     df = pd.DataFrame(pd.read_csv(full_name,index_col=0)[col_name])\n",
    "#     name = f[:f.rfind('.')]\n",
    "#     df = df.rename(columns={col_name: name})    \n",
    "#     futures_df = pd.concat([futures_df,df],axis=1).reindex(futures_df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ****************************Adjustments****************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes \n",
    " 1. When you go live you need to make respective adjustments since the algo will run on the same day "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Open Interest & Volume Not Available for yesterday so after shifting and bfill it shows same values for 2 dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of daily_dataframe\n",
    "daily_dataframe_main = daily_dataframe.copy()\n",
    "\n",
    "# Converting to MultiIndex\n",
    "daily_dataframe_main.columns = ['LB_High','LB_Low','LB_Close','LB_Volume','LB_openInterest', 'LB_cash_price']\n",
    "cols = pd.MultiIndex.from_product([['Lumber_OHLCV'], (daily_dataframe_main.columns)])\n",
    "daily_dataframe_main = pd.DataFrame(data=daily_dataframe_main.values, index=daily_dataframe_main.index, columns=cols)\n",
    "\n",
    "daily_dataframe_main.head(4).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Lumber Contract Spreads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to MultiIndex\n",
    "col = ['First-Second','Second-Third','First-Third']\n",
    "lumber_contract_spreads.columns = pd.MultiIndex.from_product([['Lumber_Contract_Spreads'], (col)])\n",
    "lumber_contract_spreads = pd.DataFrame(data=lumber_contract_spreads.values, index=lumber_contract_spreads.index, columns=lumber_contract_spreads.columns)\n",
    "lumber_contract_spreads = lumber_contract_spreads.loc[~lumber_contract_spreads.index.duplicated(keep='first')]\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main,lumber_contract_spreads], axis=1).reindex(daily_dataframe_main.index)\n",
    "daily_dataframe_main.head(4).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Lumber Moving Averages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to MultiIndex\n",
    "lumber_moving_averages.columns = pd.MultiIndex.from_product([['lumber_moving_averages'], (lumber_moving_averages.columns)])\n",
    "lumber_moving_averages = pd.DataFrame(data=lumber_moving_averages.values, index=lumber_moving_averages.index, columns=lumber_moving_averages.columns)\n",
    "lumber_moving_averages = lumber_moving_averages.loc[~lumber_moving_averages.index.duplicated(keep='first')]\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main,lumber_moving_averages], axis=1).reindex(daily_dataframe_main.index)\n",
    "daily_dataframe_main.head(4).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining adjusted DataFrames based on switching Logic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to MultiIndex\n",
    "adj_df.columns = pd.MultiIndex.from_product([['Lumber_Adjusted_dfs'], (adj_df.columns)])\n",
    "\n",
    "adj_df = pd.DataFrame(data=adj_df.values, index=adj_df.index, columns=adj_df.columns)\n",
    "adj_df = adj_df.loc[~adj_df.index.duplicated(keep='first')]\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main,adj_df], axis=1).reindex(daily_dataframe_main.index)\n",
    "daily_dataframe_main.head(4).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BackFilling & Shifting Up so yesterday's price etc is lined up as Today's price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all Non Traded Days put value of previous day \n",
    "daily_dataframe_main.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# USING Previous Day's High,Low,Close Price inorder to predict today's price\n",
    "daily_dataframe_main = daily_dataframe_main.shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Investingdotcom DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investing_df = pd.DataFrame(index= daily_dataframe_main.index)\n",
    "\n",
    "for k in Investing_dict.keys():\n",
    "    investing_single_df = pd.DataFrame(Investing_dict[k].reindex(daily_dataframe_main.index).fillna(method='bfill')['Actual'])\n",
    "    investing_single_df.columns = [k]\n",
    "    investing_df = pd.concat([investing_df,investing_single_df], axis=1)\n",
    "\n",
    "try:\n",
    "    investing_df.columns = pd.MultiIndex.from_product([['Investingdotcom'], (investing_df.columns)])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "investing_df = pd.DataFrame(data=investing_df.values, index=investing_df.index, columns=investing_df.columns)\n",
    "investing_df = investing_df.loc[~investing_df.index.duplicated(keep='first')]\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main,investing_df], axis=1).reindex(daily_dataframe_main.index)\n",
    "daily_dataframe_main.head(4).append(daily_dataframe_main.tail(2))\n",
    "\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining NAHB/Wells Fargo Housing Market Index (HMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 12 business days to the release date since the HMI is usually published on the 12th Business day of the month but sometimes on 11th or 13th\n",
    "wells_hmi_df.index = wells_hmi_df.index.to_series().apply(lambda x: pd.date_range(x, periods=12, freq=\"B\")[-1])\n",
    "wells_hmi_df.sort_index(ascending = False, inplace=True)\n",
    "\n",
    "# If by chance the calculated date is more than the current date, we limit the date to current date only since we are appending 12 days to 1st of every month\n",
    "min_date = min(wells_hmi_df.index[0], datetime.now())\n",
    "wells_hmi_df.rename(index={wells_hmi_df.index[0]:min_date},inplace=True)\n",
    "wells_hmi_df = wells_hmi_df.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "\n",
    "# Combining with daily_dataframe_main\n",
    "wells_hmi_df.columns = pd.MultiIndex.from_product([['Wells Fargo Housing Market Index'], (wells_hmi_df.columns)])\n",
    "wells_hmi_df = pd.DataFrame(data=wells_hmi_df.values, index=wells_hmi_df.index, columns=wells_hmi_df.columns)\n",
    "wells_hmi_df = wells_hmi_df.loc[~wells_hmi_df.index.duplicated(keep='first')]\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main,wells_hmi_df], axis=1).reindex(daily_dataframe_main.index)\n",
    "daily_dataframe_main.head(4).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining OverNight Funding Rates - NEWYORK FED     \n",
    "- Use the if condition when going Live inorder to get the latest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First Aligning it with Daily Prices and then combining with daily_dataframe_main\n",
    "over_night_rates_df = over_night_rates_df.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "\n",
    "# Only Use this when going Live inorder to get this morning's data \n",
    "# Only Shift up if even one series is NaN - basically data gets populated in the morning for all except one series which gets populated a day before after market close\n",
    "# if (np.isnan(over_night_rates_df.loc[over_night_rates_df.index[0]].values.tolist()).any()): # Returns True if even 1 is NaN - Otherwise False - If True then shift up otherwise use current data\n",
    "\n",
    "\n",
    "# EFFR (assuming same for all other rates) published everyday at ~9AM for the prior business day so shifting the dataframe up by 1 day\n",
    "over_night_rates_df = over_night_rates_df.shift(-1)\n",
    "\n",
    "# Joining with daily data\n",
    "cols = pd.MultiIndex.from_product([['OverNight_Rates'], (over_night_rates_df.columns)])\n",
    "over_night_rates_df = pd.DataFrame(data=over_night_rates_df.values, index=over_night_rates_df.index, columns=cols)\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, over_night_rates_df], axis=1).reindex(daily_dataframe_main.index)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining pierian Interest Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Aligning it with Daily Prices and then combining with daily_dataframe_main\n",
    "interest_rate_pierian_df = interest_rate_pierian_df.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "\n",
    "# published everyday at ~time? for the prior business day so shifting the dataframe up by 1 day\n",
    "interest_rate_pierian_df = interest_rate_pierian_df.shift(-1)\n",
    "\n",
    "# Joining with daily data\n",
    "cols = pd.MultiIndex.from_product([['Pierian_InterestRates'], (interest_rate_pierian_df.columns)])\n",
    "interest_rate_pierian_df = pd.DataFrame(data=interest_rate_pierian_df.values, index=interest_rate_pierian_df.index, columns=cols)\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, interest_rate_pierian_df], axis=1).reindex(daily_dataframe_main.index)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining  weekly_realtor\n",
    "- Weekly Data released on Thursday (Check if data released before marked open otherwise it should be Friday) for last Saturday - now since it wasn't available on Saturday, we need to pull it up so it is against Thursday of Next Week and Not Saturday of previous week because then the data wasn't even available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data came out on a Thursday we make the index shift up to Thursday (Check if data released before marked open otherwise it should be Friday)\n",
    "weekly_realtor = pd.DataFrame(index= pd.DatetimeIndex(weekly_realtor.index),data = weekly_realtor.values, columns=weekly_realtor.columns ).resample('W-Thu').mean().sort_index(ascending=False)\n",
    "\n",
    "# But if by chance the calculated date for Thursday is more than the current date, we limit the date to current date only\n",
    "min_date = min(weekly_realtor.index[0], datetime.now())\n",
    "weekly_realtor.rename(index={weekly_realtor.index[0]:min_date},inplace=True)\n",
    "\n",
    "# Joining with daily data\n",
    "cols = pd.MultiIndex.from_product([['Weekly_realtor'], (weekly_realtor.columns)])\n",
    "weekly_realtor = pd.DataFrame(data=weekly_realtor.values, index=weekly_realtor.index, columns=cols).reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, weekly_realtor], axis=1)\n",
    "daily_dataframe_main.head(7).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining  monthly_realtor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically Monthly Realtor data releases with a lag of 30-32 days so pulling it up by 1 MONTH \"Monthly data updated on June 2, 2022 with data through May 2022. Next update scheduled for June 30, 2022 with data through June 2022.È\n",
    "monthly_realtor = monthly_realtor.shift(1, freq=\"MS\")\n",
    "monthly_realtor.sort_index(ascending = False, inplace=True)\n",
    "\n",
    "# Joining with daily data\n",
    "cols = pd.MultiIndex.from_product([['monthly_realtor'], (monthly_realtor.columns)])\n",
    "monthly_realtor = pd.DataFrame(data=monthly_realtor.values, index=monthly_realtor.index, columns=cols)\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, monthly_realtor], axis=1).reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining GSCPI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"We update the GSCPI at 10:00 a.m. on the fourth business day of each month.\"\" For example, April data is updated on the fourth business day of May. Also since the data is reported with end of month date, we simply do pull the data by 4+1 business days \n",
    "GSCPI.index = GSCPI.index.to_series().apply(lambda x: pd.date_range(x, periods=5, freq=\"B\")[-1])\n",
    "\n",
    "# Joining with daily data\n",
    "cols = pd.MultiIndex.from_product([['GSCPI'], (GSCPI.columns)])\n",
    "GSCPI = pd.DataFrame(data=GSCPI.values, index=GSCPI.index, columns=cols)\n",
    "GSCPI.sort_index(ascending = False, inplace=True)\n",
    "GSCPI = GSCPI.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, GSCPI], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining FTR ETF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receive on Monday for Week ending on Friday - But This is Daily Data - so delay by 3 business days (because at eod)\n",
    "FTR.index = FTR.index.to_series().apply(lambda x: pd.date_range(x, periods=3, freq=\"B\")[-1])\n",
    "\n",
    "# But if by chance the calculated date is more than the current date, we limit the date to current date only\n",
    "min_date = min(FTR.index[0], datetime.now())\n",
    "FTR.rename(index={FTR.index[0]:min_date},inplace=True)\n",
    "\n",
    "# Joining with daily data\n",
    "cols = pd.MultiIndex.from_product([['FTR'], (FTR.columns)])\n",
    "FTR = pd.DataFrame(data=FTR.values, index=FTR.index, columns=cols)\n",
    "\n",
    "FTR = FTR.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, FTR], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Weekly Economic Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May 28th Data (Saturday) will come out on 2nd June (Next Thursday) & I think data comes out at 11:30. Updates on Thursday at 11:30 am EST for the week ending before so thursday will have impact on Thursday Price  https://www.newyorkfed.org/research/policy/weekly-economic-index#/interactive\n",
    "WEI = pd.DataFrame(index= pd.DatetimeIndex(WEI.index),data = WEI.values, columns=WEI.columns ).resample('W-Thu').mean().sort_index(ascending=False)\n",
    "\n",
    "# But if by chance the calculated date for Thursday is more than the current date, we limit the date to current date only\n",
    "min_date = min(WEI.index[0], datetime.now())\n",
    "WEI.rename(index={WEI.index[0]:min_date},inplace=True)\n",
    "\n",
    "# Joining with daily data\n",
    "cols = pd.MultiIndex.from_product([['Weekly Economic Index'], (WEI.columns)])\n",
    "WEI = pd.DataFrame(data=WEI.values, index=WEI.index, columns=cols)\n",
    "\n",
    "WEI = WEI.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, WEI], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Yield Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is similar to GSCPI - COPYING FROM NOTES ABOVE - # \"We update the GSCPI at 10:00 a.m. on the fourth business day of each month.\"\" For example, April data is updated on the fourth business day of May. Also since the data is reported with end of month date, we simply do forward fill and will later put a lag of 4 (+1 - for last day of month) business days\n",
    "\n",
    "Y_curve.index = Y_curve.index.to_series().apply(lambda x: pd.date_range(x, periods=5, freq=\"B\")[-1])\n",
    "\n",
    "# Joining with daily data\n",
    "cols = pd.MultiIndex.from_product([['Y_curve'], (Y_curve.columns)])\n",
    "Y_curve = pd.DataFrame(data=Y_curve.values, index=Y_curve.index, columns=cols)\n",
    "Y_curve.sort_index(ascending = False, inplace=True)\n",
    "Y_curve = Y_curve.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, Y_curve], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Treasury Term Premia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving up by 1 day since 1 day ago treasuries will impact today's prices\n",
    "Treasury_Terms = Treasury_Terms.shift(-1)\n",
    "\n",
    "# Joining with daily data\n",
    "cols = pd.MultiIndex.from_product([['Treasury_Terms'], (Treasury_Terms.columns)])\n",
    "Treasury_Terms = pd.DataFrame(data=Treasury_Terms.values, index=Treasury_Terms.index, columns=cols)\n",
    "Treasury_Terms.sort_index(ascending = False, inplace=True)\n",
    "Treasury_Terms = Treasury_Terms.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, Treasury_Terms], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Yahoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_close_yahoo =  data_close_yahoo.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "\n",
    "# Moving up by 1 day since 1 day ago stock prices will impact today's prices\n",
    "data_close_yahoo = data_close_yahoo.shift(-1)\n",
    "\n",
    "# Joining with daily data\n",
    "cols = pd.MultiIndex.from_product([['Yahoo'], (data_close_yahoo.columns)])\n",
    "\n",
    "data_close_yahoo = pd.DataFrame(data=data_close_yahoo.values, index=data_close_yahoo.index, columns=cols)\n",
    "data_close_yahoo.sort_index(ascending = False, inplace=True)\n",
    "data_close_yahoo = data_close_yahoo.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, data_close_yahoo], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining CN Carloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember the dates are for the week starting - CN Data index is given as Sunday and is for the next week. But the data comes out on Monday (assuming same as CP) with a week lag\n",
    "# Thus we need to pull up series by 8 days to next Monday (also it comes before market open I think)\n",
    "cn_carloads.index = cn_carloads.index + pd.DateOffset(days=8)\n",
    "\n",
    "# But if by chance the calculated date  is more than the current date, we limit the date to current date only\n",
    "min_date = min(cn_carloads.index[0], datetime.now())\n",
    "cn_carloads.rename(index={cn_carloads.index[0]:min_date},inplace=True)\n",
    "\n",
    "# Joining with daily data\n",
    "cols = pd.MultiIndex.from_product([['Canadian_National_Railway'], (cn_carloads.columns)])\n",
    "cn_carloads = pd.DataFrame(data=cn_carloads.values, index=cn_carloads.index, columns=cols)\n",
    "\n",
    "cn_carloads = cn_carloads.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, cn_carloads], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining CP Carloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP carloads are for week ending and given on Saturday so we add 2 days to get to Monday since thats when data became available. Assuming comes out b4 market open\n",
    "cp_carloads.index = cp_carloads.index + pd.DateOffset(days=2)\n",
    "\n",
    "# But if by chance the calculated date  is more than the current date, we limit the date to current date only\n",
    "min_date = min(cp_carloads.index[0], datetime.now())\n",
    "cp_carloads.rename(index={cp_carloads.index[0]:min_date},inplace=True)\n",
    "\n",
    "# Joining with daily data & Using SAME column names as CN carloads\n",
    "cp_carloads.columns = ['Primary Forest Prods','Lumber & Wood Prods','Pulp & Paper Prods','Total Forest Products']\n",
    "\n",
    "cols = pd.MultiIndex.from_product([['Canadian_Pacific'], (cp_carloads.columns)])\n",
    "cp_carloads = pd.DataFrame(data=cp_carloads.values, index=cp_carloads.index, columns=cols)\n",
    "cp_carloads = cp_carloads.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, cp_carloads], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining CN & CP Carloads for Lumber & Wood Prods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_dataframe_main.loc[:,('Canada Rail Lumber & Wood Prods','Lumber & Wood Prods')] = daily_dataframe_main.loc[:, ('Canadian_National_Railway', 'Lumber & Wood Prods')].fillna(0) + daily_dataframe_main.loc[:, ('Canadian_Pacific', 'Lumber & Wood Prods')].fillna(0)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining CP Train Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP carloads are for week ending and given on Friday so we add 3 days to get to Monday since thats when data became available. Assuming comes out b4 market open\n",
    "cp_train_speed.index = cp_train_speed.index + pd.DateOffset(days=3)\n",
    "\n",
    "# But if by chance the calculated date  is more than the current date, we limit the date to current date only\n",
    "min_date = min(cp_train_speed.index[0], datetime.now())\n",
    "cp_train_speed.rename(index={cp_train_speed.index[0]:min_date},inplace=True)\n",
    "\n",
    "cols = pd.MultiIndex.from_product([['CP_Train_Speed'], (cp_train_speed.columns)])\n",
    "cp_train_speed = pd.DataFrame(data=cp_train_speed.values, index=cp_train_speed.index, columns=cols)\n",
    "cp_train_speed = cp_train_speed.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, cp_train_speed], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining CP Terminal Dwell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CP carloads are for week ending and given on Friday so we add 3 days to get to Monday since thats when data became available. Assuming comes out b4 market open\n",
    "cp_terminal_dwell.index = cp_terminal_dwell.index + pd.DateOffset(days=3)\n",
    "\n",
    "# But if by chance the calculated date  is more than the current date, we limit the date to current date only\n",
    "min_date = min(cp_terminal_dwell.index[0], datetime.now())\n",
    "cp_terminal_dwell.rename(index={cp_terminal_dwell.index[0]:min_date},inplace=True)\n",
    "\n",
    "cols = pd.MultiIndex.from_product([['CP_Terminal_Dwell'], (cp_terminal_dwell.columns)])\n",
    "cp_terminal_dwell = pd.DataFrame(data=cp_terminal_dwell.values, index=cp_terminal_dwell.index, columns=cols)\n",
    "cp_terminal_dwell = cp_terminal_dwell.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, cp_terminal_dwell], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Fred all Frequency Data \n",
    "- Note since we are putting a condition to update only those series which were updated/release after 2022 feb, annual and quarterly data won't be updated in addition to any discontinued series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_format_fred_1 = \"%Y-%m-%d %H:%M:%S-%f\"\n",
    "date_format_fred_2 = \"%Y-%m-%d\"\n",
    "\n",
    "# fred_weekly_freq = ['Daily','Monthly','Weekly, As of Wednesday','Weekly, Ending Friday','Weekly, Ending Monday','Weekly, Ending Saturday','Weekly, Ending Thursday','Weekly, Ending Wednesday']\n",
    "frequency_count = fred_desc.loc[['id','frequency']].T.groupby('frequency').count()\n",
    "fred_weekly_freq = frequency_count.index.to_list()\n",
    "\n",
    "for fred_frequency in fred_weekly_freq:\n",
    "\n",
    "    FRED_FREQ_DF = pd.DataFrame(index=daily_dataframe_main.index)\n",
    "    fred_db_tables[fred_frequency].sort_index(ascending = False, inplace=True)\n",
    "    \n",
    "    # Has Lots of 0 so replacing them with previous values\n",
    "    fred_db_tables[fred_frequency].replace(to_replace=0, method='bfill', inplace=True) \n",
    "        \n",
    "\n",
    "    for col in fred_db_tables[fred_frequency].columns:    \n",
    "        tick = fred_desc.apply(lambda row: row[row == col].index, axis=1)['title'][0]\n",
    "        last_updated = fred_desc[tick]['last_updated']\n",
    "        first_index = fred_db_tables[fred_frequency][col].first_valid_index()\n",
    "        date_update = datetime.strptime(last_updated, date_format_fred_1).date()\n",
    "        hour_update = datetime.strptime(last_updated, date_format_fred_1).hour\n",
    "        # date_valid = datetime.strptime(first_index, date_format_fred_2).date()\n",
    "        date_valid = first_index.date()\n",
    "        diff_in_days = (date_update - date_valid).days\n",
    "        if hour_update > 15: diff_in_days += 1 # move to next day if update is after 3:00pm\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        df_index = pd.DatetimeIndex(fred_db_tables[fred_frequency][col].index) + pd.DateOffset(days=diff_in_days)\n",
    "        df = pd.DataFrame(data=fred_db_tables[fred_frequency][col].values, index=df_index, columns=[col])\n",
    "        \n",
    "        # only if last available date is more than Feb 2022 then we add it to the frame - so I think Quarterly and Annual won't be added\n",
    "        if date_valid.year > 2021 and date_valid.month > 2:\n",
    "            FRED_FREQ_DF = pd.DataFrame(data=pd.concat([FRED_FREQ_DF, df], axis=1), index=FRED_FREQ_DF.index)\n",
    "            FRED_FREQ_DF = FRED_FREQ_DF.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "\n",
    "    FRED_FREQ_DF.sort_index(ascending = False, inplace=True)\n",
    "\n",
    "    # But if by chance the calculated date is more than the current date, we limit the date to current date only\n",
    "    min_date = min(FRED_FREQ_DF.index[0], datetime.now())\n",
    "    FRED_FREQ_DF.rename(index={FRED_FREQ_DF.index[0]:min_date},inplace=True)\n",
    "\n",
    "    # Joining with daily data\n",
    "    Group_name = str(\"FRED_\" + fred_frequency)\n",
    "    cols = pd.MultiIndex.from_product([[Group_name], (FRED_FREQ_DF.columns)])\n",
    "    FRED_FREQ_DF = pd.DataFrame(data=FRED_FREQ_DF.values, index=FRED_FREQ_DF.index, columns=cols)\n",
    "    daily_dataframe_main = pd.concat([daily_dataframe_main, FRED_FREQ_DF], axis=1)\n",
    "\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining US Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_Census_schedule = get_us_census_schedule()\n",
    "us_census_dict = {'New Residential Construction':new_residential_construction_df,\n",
    "'Construction Spending':construct_spend_df,\n",
    "'New Home Sales':new_home_sales_df,\n",
    "'Monthly Wholesale Trade: Sales and Inventories':monthly_wholesale_trade_df}\n",
    "\n",
    "US_Census_schedule = get_us_census_schedule().sort_values(by='Release Date',ascending=False)\n",
    "\n",
    "for key in us_census_dict.keys():\n",
    "    indicator_schedule_df = US_Census_schedule[US_Census_schedule.Indicator == key]\n",
    "    \n",
    "    # Avg difference between release date and period covered by the data\n",
    "    avg_days_diff = indicator_schedule_df[['Period Covered','Release Date']].diff(axis=1)['Release Date'].mean()\n",
    "\n",
    "    # Slice the release schedule dataframe to only indices present in both the dataframes & rename indices\n",
    "    indicator_schedule_df = indicator_schedule_df[indicator_schedule_df['Period Covered'].isin(us_census_dict[key].index)]\n",
    "    previous_index = us_census_dict[key].iloc[0:len(indicator_schedule_df)].index\n",
    "    revised_index = indicator_schedule_df['Release Date']\n",
    "    us_census_dict[key].rename(index= {x:y for x,y in zip(previous_index,revised_index)}, inplace=True)\n",
    "    \n",
    "    # For older indices, just add the average number of days between releases and period covered\n",
    "    previous_index = us_census_dict[key].iloc[len(indicator_schedule_df):].index\n",
    "    revised_index = pd.DatetimeIndex(us_census_dict[key].iloc[len(indicator_schedule_df):].index) + pd.DateOffset(days=avg_days_diff.days)\n",
    "    us_census_dict[key].rename(index= {x:y for x,y in zip(previous_index,revised_index)}, inplace=True)\n",
    "\n",
    "    # Concatenating with Main dataframe\n",
    "    cols = pd.MultiIndex.from_product([[key], (us_census_dict[key].columns)])\n",
    "    df = pd.DataFrame(data=us_census_dict[key].values, index=us_census_dict[key].index, columns=cols)\n",
    "\n",
    "    df = df.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "    df.sort_index(ascending = False, inplace=True)\n",
    "    daily_dataframe_main = pd.concat([daily_dataframe_main, df], axis=1)\n",
    "\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Barometer Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BarometerFinishedInventories - is Monthly, Rest Are weekly\n",
    "\n",
    "barometer_data_dict = {}\n",
    "Barometer_list = list(update_df[update_df['report_name'] == 'Barometer']['model'].values)\n",
    "\n",
    "Barometer = pd.DataFrame()\n",
    "\n",
    "for name in Barometer_list:\n",
    "    model = get_class_by_tablename(name)\n",
    "    data=query_data(wwpa_ssn,model)\n",
    "    model_data = pd.DataFrame(data)\n",
    "    model_data.sort_values(by=['Timestamp'],ascending=False,inplace=True)\n",
    "    model_data.index = model_data['Timestamp']\n",
    "    model_data.index = pd.DatetimeIndex(model_data.index)\n",
    "    model_data.drop(['Timestamp','Month','Day','File','Year'],axis=1,inplace=True)\n",
    "    barometer_data_dict[model.__tablename__] = model_data\n",
    "\n",
    "    cols = pd.MultiIndex.from_product([[model.__tablename__], (barometer_data_dict[model.__tablename__].columns)])\n",
    "    barometer_data_dict[model.__tablename__].columns = cols\n",
    "    barometer_data_dict[model.__tablename__] = barometer_data_dict[model.__tablename__][~barometer_data_dict[model.__tablename__].index.duplicated(keep='first')]\n",
    "    barometer_data_dict[model.__tablename__].sort_index(ascending = False, inplace=True)\n",
    "    Barometer = pd.concat([Barometer, barometer_data_dict[model.__tablename__]], axis=1)\n",
    "    Barometer.sort_index(ascending = False, inplace=True)\n",
    "\n",
    "# For previous week Saturday (reported index) available end of market Thursday - so increasing index by 6 days - i.e for Friday close\n",
    "Barometer.index = Barometer.index + pd.DateOffset(days=6)\n",
    "Barometer = Barometer.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, Barometer], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Lumber Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lumber_Track_dict = {}\n",
    "Lumber_Track_list = list(update_df[update_df['report_name'] == 'Lumber Track']['model'].values)\n",
    "Lumber_Track = pd.DataFrame()\n",
    "\n",
    "for name in Lumber_Track_list:\n",
    "    model = get_class_by_tablename(name)\n",
    "    data=query_data(wwpa_ssn,model)\n",
    "    model_data = pd.DataFrame(data)\n",
    "    model_data.sort_values(by=['Timestamp'],ascending=False,inplace=True)\n",
    "    model_data.index = model_data['Timestamp']\n",
    "    model_data.index = pd.DatetimeIndex(model_data.index)\n",
    "    model_data.drop(['Timestamp','Month','File','Year'],axis=1,inplace=True)\n",
    "    Lumber_Track_dict[model.__tablename__] = model_data\n",
    "\n",
    "    cols = pd.MultiIndex.from_product([[model.__tablename__], (Lumber_Track_dict[model.__tablename__].columns)])\n",
    "    Lumber_Track_dict[model.__tablename__].columns = cols\n",
    "    Lumber_Track_dict[model.__tablename__] = Lumber_Track_dict[model.__tablename__][~Lumber_Track_dict[model.__tablename__].index.duplicated(keep='first')]\n",
    "    Lumber_Track_dict[model.__tablename__].sort_index(ascending = False, inplace=True)\n",
    "    Lumber_Track = pd.concat([Lumber_Track, Lumber_Track_dict[model.__tablename__]], axis=1)\n",
    "    Lumber_Track.sort_index(ascending = False, inplace=True)\n",
    "\n",
    "Lumber_Track.columns.set_levels(['Lumber_Track_' + str(x) for x in Lumber_Track.columns.levels[0]],level=0,inplace=True)\n",
    "\n",
    "# Approx difference btw reported and available date is (3 months, 10 days) so added 3 months and 10 days to each date index\n",
    "Lumber_Track.index = Lumber_Track.index + pd.DateOffset(days=10, months=3)\n",
    "\n",
    "Lumber_Track.replace(to_replace=0, method='bfill', inplace=True)\n",
    "Lumber_Track = Lumber_Track.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, Lumber_Track], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Western Lumber Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Western_Lumber_dict = {}\n",
    "Western_Lumber_list = list(update_df[update_df['report_name'] == 'Western Lumber Facts']['model'].values)\n",
    "Western_Lumber = pd.DataFrame()\n",
    "\n",
    "for name in Western_Lumber_list:\n",
    "    model = get_class_by_tablename(name)\n",
    "    data=query_data(wwpa_ssn,model)\n",
    "    model_data = pd.DataFrame(data)\n",
    "    model_data.sort_values(by=['Timestamp'],ascending=False,inplace=True)\n",
    "    model_data.index = model_data['Timestamp']\n",
    "    model_data.index = pd.DatetimeIndex(model_data.index)\n",
    "    model_data.drop(['Timestamp','Month','File','Year'],axis=1,inplace=True)\n",
    "    Western_Lumber_dict[model.__tablename__] = model_data\n",
    "\n",
    "    cols = pd.MultiIndex.from_product([[model.__tablename__], (Western_Lumber_dict[model.__tablename__].columns)])\n",
    "    Western_Lumber_dict[model.__tablename__].columns = cols\n",
    "    Western_Lumber_dict[model.__tablename__] = Western_Lumber_dict[model.__tablename__][~Western_Lumber_dict[model.__tablename__].index.duplicated(keep='first')]\n",
    "    Western_Lumber_dict[model.__tablename__].sort_index(ascending = False, inplace=True)\n",
    "    Western_Lumber = pd.concat([Western_Lumber, Western_Lumber_dict[model.__tablename__]], axis=1)\n",
    "    Western_Lumber.sort_index(ascending = False, inplace=True)\n",
    "\n",
    "Western_Lumber.columns.set_levels(['Western_Lumber_' + str(x) for x in Western_Lumber.columns.levels[0]],level=0,inplace=True)\n",
    "\n",
    "# Approx difference btw reported and available date is (2 months, 8 days) so added 2 months and 8 days to each date index\n",
    "Western_Lumber.index = Western_Lumber.index + pd.DateOffset(days=8, months=2)\n",
    "\n",
    "Western_Lumber.replace(to_replace=0, method='bfill', inplace=True)\n",
    "Western_Lumber = Western_Lumber.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, Western_Lumber], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining COT Data - Both dissaggregated & legacy - using futures_plus_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from CommitmentsOfTraders.backend.db.models import *\n",
    "cot_db_url=\"postgresql://fanjum:3cUDV4jctsfF0pYcxoMQ@pg-prod-master.data-191214.com:5432/lumber-data-sources\"\n",
    "cot_schema=\"cot\"\n",
    "cot_ssn=connect_to_database(cot_db_url, cot_schema)\n",
    "\n",
    "\n",
    "def cot_old_new(dis_or_leg,fut_or_futopt):\n",
    "    \n",
    "    report_type =  str(dis_or_leg + '_' + fut_or_futopt)\n",
    "\n",
    "    if dis_or_leg == 'disaggregated':\n",
    "        model_name =  DisaggregatedFuturesOptions\n",
    "        columns_to_keep =['Date','Open_Interest_All','Prod_Merc_Positions_Long_All','Prod_Merc_Positions_Short_All','M_Money_Positions_Long_All','M_Money_Positions_Short_All','M_Money_Positions_Spread_All','Tot_Rept_Positions_Long_All','Tot_Rept_Positions_Short_All','Change_in_Open_Interest_All','Change_in_Prod_Merc_Long_All','Change_in_Prod_Merc_Short_All','Change_in_M_Money_Long_All','Change_in_M_Money_Short_All','Pct_of_Open_Interest_All','Pct_of_OI_Prod_Merc_Long_All','Pct_of_OI_Prod_Merc_Short_All','Pct_of_OI_M_Money_Long_All','Pct_of_OI_M_Money_Short_All','Pct_of_OI_M_Money_Spread_All']\n",
    "        data=query_data(cot_ssn,model_name)\n",
    "        model_data = pd.DataFrame(data)\n",
    "        model_data = model_data[model_data.Report_Type == report_type]\n",
    "\n",
    "        model_data =model_data[columns_to_keep]\n",
    "        model_data['M_Money_Positions_Long_All'] = model_data['M_Money_Positions_Long_All'].astype(int)\n",
    "        model_data['M_Money_Positions_Short_All'] = model_data['M_Money_Positions_Short_All'].astype(int)\n",
    "        model_data['Net_Spec_Length'] = model_data['M_Money_Positions_Long_All'] - model_data['M_Money_Positions_Short_All']\n",
    "        model_data['Pct_of_OI_MM_NSL'] = model_data['Pct_of_OI_M_Money_Long_All'] - model_data['Pct_of_OI_M_Money_Short_All']\n",
    "\n",
    "    elif dis_or_leg == 'legacy':\n",
    "        model_name =  Legacy\n",
    "        data=query_data(cot_ssn,model_name)\n",
    "        model_data = pd.DataFrame(data)\n",
    "        model_data = model_data[model_data.Report_Type == report_type]\n",
    "        model_data.drop(['Report_Type','Market_Code','Market_and_Exchange_Names','CFTC_Contract_Market_Code','CFTC_Market_Code_in_Initials','CFTC_Region_Code','CFTC_Commodity_Code','Contract_Units','CFTC_Contract_Market_Code_Quotes','CFTC_Market_Code_in_Initials_Quotes','CFTC_Commodity_Code_Quotes'],axis=1,inplace=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('disaggregated or legacy')\n",
    "\n",
    "    model_data.index = model_data.Date\n",
    "    model_data.index = pd.DatetimeIndex(model_data.index)\n",
    "    model_data.drop(['Date'],axis=1,inplace=True)\n",
    "    model_data.index = pd.DatetimeIndex(model_data.index)\n",
    "    model_data.sort_index(ascending=False,inplace=True)\n",
    "\n",
    "    return model_data\n",
    "\n",
    "# For Dissaggregated Report\n",
    "COT = cot_old_new('disaggregated','futopt')\n",
    "cols = pd.MultiIndex.from_product([['CFTC_disaggregated'], (COT.columns)])\n",
    "COT = pd.DataFrame(data=COT.values, index=COT.index, columns=cols)\n",
    "# The data released Friday at 3:30 pm est, for previous Tuesday (close). So since it doesn't impact Friday close, we move to to next Monday by adding 6 days\n",
    "COT.index = COT.index + pd.DateOffset(days=6)\n",
    "COT = COT.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "COT.sort_index(ascending = False, inplace=True)\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, COT], axis=1)\n",
    "\n",
    "# FOR Legacy Report\n",
    "COT_legacy = cot_old_new('legacy','futopt')\n",
    "cols = pd.MultiIndex.from_product([['CFTC_legacy'], (COT_legacy.columns)])\n",
    "COT_legacy = pd.DataFrame(data=COT_legacy.values, index=COT_legacy.index, columns=cols)\n",
    "# The data released Friday at 3:30 pm est, for previous Tuesday (close). So since it doesn't impact Friday close, we move to to next Monday by adding 6 days\n",
    "COT_legacy.index = COT_legacy.index + pd.DateOffset(days=6)\n",
    "COT_legacy = COT_legacy.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "COT_legacy.sort_index(ascending = False, inplace=True)\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, COT_legacy], axis=1)\n",
    "\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining FEA - Only Select Indices from Lumber Advisor Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from FEA.backend.db.models import * \n",
    "\n",
    "fea_db_url=\"postgresql://fanjum:3cUDV4jctsfF0pYcxoMQ@pg-prod-master.data-191214.com:5432/2x4\"\n",
    "fea_schema=\"fea\"\n",
    "fea_ssn=connect_to_database(fea_db_url, fea_schema)\n",
    "mapper =  ProductmonthlylumberadvisorSeriesList\n",
    "lumberadvisor=query_data(fea_ssn,mapper=mapper)\n",
    "lumberadvisor=pd.DataFrame(lumberadvisor)\n",
    "\n",
    "# This has to be put Manually each month once the report is out - Remember to pass this as an argument to the BIG AGGREGATE function\n",
    "historical_month_end = '2022-06-01' # Open excel at at https://getfea.com/publication/lumber-service/monthly-lumber-advisor and the last NON highlighted row is actuals\n",
    "\n",
    "# Updated_on = '2022-05-27' # found at at https://getfea.com/publication/lumber-service/monthly-lumber-advisor\n",
    "# Updated_on = datetime.strptime(Updated_on, '%Y-%m-%d').date()\n",
    "\n",
    "# Once the database is getting updated we can use the following line - no need to update manually (remove above 2 lines)\n",
    "Updated_on = datetime.strptime(lumberadvisor['updated_at'][0], \"%Y-%m-%dT%H:%M:%S+00:00\").date()\n",
    "historical_month_end = datetime.strptime(historical_month_end, '%Y-%m-%d').date()\n",
    "diff_days = (Updated_on - historical_month_end).total_seconds() / 60 / 60 / 24 \n",
    "\n",
    "lumberadvisor = lumberadvisor.pivot_table(index=['date'],columns=['description'],values=['value']).sort_index(ascending=False)\n",
    "lumberadvisor.index = pd.DatetimeIndex(lumberadvisor.index)\n",
    "fea_balance_xl_link = 'F:/Traders/2x4/2x4 v2/Data/Fundamentals/Forest Economic Advisors/Lumber_Balance_FEA.xlsx'\n",
    "fea_balance_xl = pd.read_excel(fea_balance_xl_link, sheet_name='Index')\n",
    "fea_balance_col_list = fea_balance_xl['Description'].replace('\\n', '').tolist()\n",
    "\n",
    "lumberadvisor = lumberadvisor.loc[historical_month_end:,('value',fea_balance_col_list)]\n",
    "lumberadvisor.index = lumberadvisor.index + pd.DateOffset(days=diff_days) # Manually entering 1 date (historical_month_end/last actual month reported) & then subtracting it from updated on (same as updated_at in db I think) and the diff is then added to the index\n",
    "\n",
    "lumberadvisor.columns.set_levels(['FEA_Lumber_Advisor_' + str(x) for x in lumberadvisor.columns.levels[0]],level=0,inplace=True)\n",
    "\n",
    "lumberadvisor = lumberadvisor.reindex(daily_dataframe_main.index).fillna(method='bfill')\n",
    "daily_dataframe_main = pd.concat([daily_dataframe_main, lumberadvisor], axis=1)\n",
    "daily_dataframe_main.head(2).append(daily_dataframe_main.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************Get Reponse/Dependent variable**************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changed Adj_close_ret to reflect new Adjustments to futures (from 1st to contract end - using 2nd month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['close2', 'close3','Adj_ret_roll2', 'Adj_ret_roll3']\n",
    "adj_df_abridged  = adj_df.copy(deep=True)\n",
    "adj_df_abridged.columns = adj_df_abridged.columns.get_level_values(1)\n",
    "adj_df_abridged = adj_df_abridged[cols_to_keep]\n",
    "adj_df_abridged.columns = ['Close_2','Close_3','Adj_ret_2', 'Adj_ret_3']\n",
    "adj_df_abridged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.DataFrame(index=daily_dataframe_main.index)\n",
    "response_df = pd.concat([response_df,adj_df_abridged ], axis=1).reindex(daily_dataframe_main.index)\n",
    "response_df = response_df.fillna(method='bfill')\n",
    "\n",
    "# conditions_adj_1  = [ response_df.Adj_ret_1 > 0, response_df.Adj_ret_1 == 0, response_df.Adj_ret_1 < 0 ]\n",
    "conditions_adj_2  = [ response_df.Adj_ret_2 > 0, response_df.Adj_ret_2 == 0, response_df.Adj_ret_2 < 0 ]\n",
    "conditions_adj_3  = [ response_df.Adj_ret_3 > 0, response_df.Adj_ret_3 == 0, response_df.Adj_ret_3 < 0 ]\n",
    "choices     = [ 1, 0, -1]\n",
    "\n",
    "# Adj_close_Up_Down_1 = pd.DataFrame(data = np.select(conditions_adj_1 , choices, default=np.nan), index = response_df.Adj_ret_1.index)\n",
    "Adj_close_Up_Down_2 = pd.DataFrame(data = np.select(conditions_adj_2 , choices, default=np.nan), index = response_df.Adj_ret_2.index)\n",
    "Adj_close_Up_Down_3 = pd.DataFrame(data = np.select(conditions_adj_3 , choices, default=np.nan), index = response_df.Adj_ret_3.index)\n",
    "\n",
    "# response_df = pd.concat([response_df,Adj_close_Up_Down_1 ], axis=1).reindex(daily_dataframe_main.index)\n",
    "response_df = pd.concat([response_df,Adj_close_Up_Down_2 ], axis=1).reindex(daily_dataframe_main.index)\n",
    "response_df = pd.concat([response_df,Adj_close_Up_Down_3 ], axis=1).reindex(daily_dataframe_main.index)\n",
    "\n",
    "new_index = list(adj_df_abridged.columns)\n",
    "# new_index.append('Adj_close_Up_Down_1')\n",
    "new_index.append('Adj_close_Up_Down_2')\n",
    "new_index.append('Adj_close_Up_Down_3')\n",
    "response_df.columns = new_index\n",
    "response_df .head(2).append(response_df.tail(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************Writing to Excel**************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes > 4 minutes\n",
    "\n",
    "# Get workbook and worksheet objects       \n",
    "writer = pd.ExcelWriter('daily_dataframe_main.xlsx')                                                                                                                  \n",
    "\n",
    "df1 = pd.DataFrame(columns=daily_dataframe_main.droplevel(1, axis=1).columns)\n",
    "dict_of_value_counts = dict(df1.columns.to_series().value_counts())\n",
    "\n",
    "df2 = daily_dataframe_main.droplevel(0, axis=1)\n",
    "df1.to_excel(writer, sheet_name='Variables')\n",
    "df2.to_excel(writer, sheet_name='Variables', merge_cells = True, startrow=1)\n",
    "\n",
    "workbook  = writer.book\n",
    "worksheet = writer.sheets['Variables']\n",
    "merge_format = workbook.add_format({'align': 'center'})\n",
    "\n",
    "cell_format = workbook.add_format({'align': 'center',\n",
    "                                   'valign': 'vcenter',\n",
    "                                   'border': 1,\n",
    "                                   'font_name': 'Arial', \n",
    "                                   'font_size': 12, \n",
    "                                   'color' : 'white', \n",
    "                                   'fg_color': '#000000',\n",
    "                                   'bold': True,\n",
    "                                   })\n",
    "\n",
    "row = 0\n",
    "i = 1\n",
    "for col in df1.columns.unique():\n",
    "    worksheet.set_column(i, i + dict_of_value_counts[col] - 1, 20)\n",
    "    # merge_range(first_row, first_col, last_row, last_col, data[, cell_format])\n",
    "    worksheet.merge_range(row, i, row, i + dict_of_value_counts[col] - 1, col, cell_format)\n",
    "    i += dict_of_value_counts[col]\n",
    "\n",
    "\n",
    "# Write to Excel file\n",
    "response_df.to_excel(writer, sheet_name='Response')\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "end_time = time.time()\n",
    "print((end_time - start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.Adj_close_Up_Down_2.value_counts()/len(response_df.Adj_close_Up_Down_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(daily_dataframe_main.loc[:,('Lumber_OHLCV','LB_Close')].copy(deep=True))\n",
    "df.columns = df.columns.get_level_values(0)\n",
    "df['Yr'] = df.index.year\n",
    "df = df.groupby('Yr').transform(lambda s: s.mean())\n",
    "df = df.pivot_table(columns=df.index.year, values='Lumber_OHLCV').T\n",
    "df.plot(figsize=(14,8), grid=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[[2019]].mean()[0]*1.08*1.08*1.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44a86d6b397dbc6b70abfa92d378adf5d728e5b92754036aace887a2a3f0f490"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
